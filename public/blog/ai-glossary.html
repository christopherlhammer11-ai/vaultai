<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>AI Glossary â€” HammerLockAI</title>
<meta name="description" content="The complete no-BS guide to AI terminology. Plain English definitions, real examples, and why each term matters.">
<meta property="og:title" content="AI Glossary â€” HammerLockAI">
<meta property="og:description" content="The complete no-BS guide to AI terminology. Plain English definitions and real examples.">
<meta property="og:url" content="https://hammerlockai.com/blog/ai-glossary.html">
<meta property="og:type" content="article">
<meta property="og:site_name" content="HammerLock AI">
<meta property="og:image" content="https://hammerlockai.com/brand/og-image.jpg">
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;700&family=Sora:wght@300;400;500;600;700&display=swap" rel="stylesheet">
<style>
:root {
  --bg-primary: #0a0a0f;
  --bg-secondary: #111118;
  --bg-card: #16161f;
  --bg-card-hover: #1c1c28;
  --accent-green: #00ff88;
  --accent-green-dim: rgba(0,255,136,0.15);
  --accent-green-glow: rgba(0,255,136,0.3);
  --accent-cyan: #00d4ff;
  --accent-amber: #ffb800;
  --text-primary: #e8e8ef;
  --text-secondary: #8888a0;
  --text-muted: #55556a;
  --border-subtle: rgba(255,255,255,0.06);
  --border-accent: rgba(0,255,136,0.2);
  --font-display: 'Sora', sans-serif;
  --font-mono: 'JetBrains Mono', monospace;
}

*, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }

html { scroll-behavior: smooth; }

body {
  font-family: var(--font-display);
  background: var(--bg-primary);
  color: var(--text-primary);
  line-height: 1.7;
  -webkit-font-smoothing: antialiased;
  overflow-x: hidden;
}

/* Noise overlay */
body::before {
  content: '';
  position: fixed;
  inset: 0;
  background: url("data:image/svg+xml,%3Csvg viewBox='0 0 256 256' xmlns='http://www.w3.org/2000/svg'%3E%3Cfilter id='n'%3E%3CfeTurbulence type='fractalNoise' baseFrequency='0.9' numOctaves='4' stitchTiles='stitch'/%3E%3C/filter%3E%3Crect width='100%25' height='100%25' filter='url(%23n)' opacity='0.03'/%3E%3C/svg%3E");
  pointer-events: none;
  z-index: 9999;
}

/* Top nav bar */
.topbar {
  position: fixed;
  top: 0; left: 0; right: 0;
  height: 56px;
  background: rgba(10,10,15,0.85);
  backdrop-filter: blur(20px);
  border-bottom: 1px solid var(--border-subtle);
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: 0 2rem;
  z-index: 1000;
}

.topbar-logo {
  font-family: var(--font-mono);
  font-weight: 700;
  font-size: 0.9rem;
  color: var(--accent-green);
  text-decoration: none;
  letter-spacing: 0.05em;
}

.topbar-logo span { color: var(--text-muted); }

.topbar-nav { display: flex; gap: 1.5rem; align-items: center; }

.topbar-nav a {
  font-size: 0.78rem;
  font-weight: 500;
  color: var(--text-secondary);
  text-decoration: none;
  letter-spacing: 0.04em;
  text-transform: uppercase;
  transition: color 0.2s;
}

.topbar-nav a:hover { color: var(--accent-green); }
.topbar-nav a.active { color: var(--accent-green); }

/* Hero */
.hero {
  margin-top: 56px;
  padding: 5rem 2rem 3rem;
  text-align: center;
  position: relative;
  overflow: hidden;
}

.hero::before {
  content: '';
  position: absolute;
  top: -200px;
  left: 50%;
  transform: translateX(-50%);
  width: 600px;
  height: 600px;
  background: radial-gradient(circle, var(--accent-green-dim) 0%, transparent 70%);
  pointer-events: none;
}

.hero-badge {
  display: inline-block;
  font-family: var(--font-mono);
  font-size: 0.7rem;
  font-weight: 500;
  color: var(--accent-green);
  background: var(--accent-green-dim);
  border: 1px solid var(--border-accent);
  padding: 0.35rem 1rem;
  border-radius: 100px;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  margin-bottom: 1.5rem;
}

.hero h1 {
  font-size: clamp(2rem, 5vw, 3.5rem);
  font-weight: 700;
  line-height: 1.15;
  letter-spacing: -0.03em;
  margin-bottom: 1rem;
  position: relative;
}

.hero h1 em {
  font-style: normal;
  background: linear-gradient(135deg, var(--accent-green), var(--accent-cyan));
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  background-clip: text;
}

.hero-sub {
  font-size: 1.05rem;
  color: var(--text-secondary);
  max-width: 560px;
  margin: 0 auto 2rem;
  font-weight: 300;
}

.hero-meta {
  font-family: var(--font-mono);
  font-size: 0.72rem;
  color: var(--text-muted);
}

/* Search */
.search-wrap {
  max-width: 600px;
  margin: 0 auto 2rem;
  position: relative;
}

.search-wrap input {
  width: 100%;
  padding: 0.85rem 1.2rem 0.85rem 2.8rem;
  background: var(--bg-card);
  border: 1px solid var(--border-subtle);
  border-radius: 12px;
  color: var(--text-primary);
  font-family: var(--font-display);
  font-size: 0.95rem;
  outline: none;
  transition: border-color 0.3s, box-shadow 0.3s;
}

.search-wrap input::placeholder { color: var(--text-muted); }

.search-wrap input:focus {
  border-color: var(--accent-green);
  box-shadow: 0 0 0 3px var(--accent-green-dim);
}

.search-icon {
  position: absolute;
  left: 1rem;
  top: 50%;
  transform: translateY(-50%);
  color: var(--text-muted);
  width: 18px;
  height: 18px;
}

/* Alpha nav */
.alpha-nav {
  max-width: 900px;
  margin: 0 auto 3rem;
  display: flex;
  flex-wrap: wrap;
  justify-content: center;
  gap: 0.4rem;
  padding: 0 2rem;
}

.alpha-nav a {
  font-family: var(--font-mono);
  font-size: 0.75rem;
  font-weight: 600;
  color: var(--text-muted);
  text-decoration: none;
  width: 34px;
  height: 34px;
  display: flex;
  align-items: center;
  justify-content: center;
  border: 1px solid var(--border-subtle);
  border-radius: 8px;
  transition: all 0.2s;
}

.alpha-nav a:hover,
.alpha-nav a.has-entries {
  color: var(--accent-green);
  border-color: var(--border-accent);
  background: var(--accent-green-dim);
}

/* Sections */
.glossary-container {
  max-width: 820px;
  margin: 0 auto;
  padding: 0 2rem 6rem;
}

.letter-section {
  margin-bottom: 3rem;
}

.letter-heading {
  font-family: var(--font-mono);
  font-size: 0.7rem;
  font-weight: 700;
  color: var(--accent-green);
  letter-spacing: 0.15em;
  text-transform: uppercase;
  padding-bottom: 0.6rem;
  margin-bottom: 1.5rem;
  border-bottom: 1px solid var(--border-accent);
  display: flex;
  align-items: center;
  gap: 0.5rem;
}

.letter-heading::before {
  content: '//';
  color: var(--text-muted);
}

/* Term cards */
.term-card {
  background: var(--bg-card);
  border: 1px solid var(--border-subtle);
  border-radius: 14px;
  padding: 1.8rem 2rem;
  margin-bottom: 1rem;
  transition: all 0.3s ease;
  cursor: pointer;
  position: relative;
  overflow: hidden;
}

.term-card::after {
  content: '';
  position: absolute;
  left: 0; top: 0; bottom: 0;
  width: 3px;
  background: var(--accent-green);
  opacity: 0;
  transition: opacity 0.3s;
}

.term-card:hover {
  border-color: var(--border-accent);
  background: var(--bg-card-hover);
  transform: translateY(-1px);
  box-shadow: 0 8px 30px rgba(0,0,0,0.3);
}

.term-card:hover::after { opacity: 1; }

.term-card.expanded { background: var(--bg-card-hover); }
.term-card.expanded::after { opacity: 1; }

.term-title {
  font-size: 1.15rem;
  font-weight: 600;
  color: var(--text-primary);
  display: flex;
  align-items: center;
  justify-content: space-between;
  gap: 0.5rem;
}

.term-title .abbr {
  font-family: var(--font-mono);
  font-size: 0.72rem;
  color: var(--accent-green);
  background: var(--accent-green-dim);
  padding: 0.2rem 0.6rem;
  border-radius: 6px;
  font-weight: 500;
}

.term-expand-icon {
  width: 20px;
  height: 20px;
  color: var(--text-muted);
  transition: transform 0.3s, color 0.3s;
  flex-shrink: 0;
}

.term-card.expanded .term-expand-icon {
  transform: rotate(180deg);
  color: var(--accent-green);
}

.term-body {
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.4s ease, opacity 0.3s;
  opacity: 0;
}

.term-card.expanded .term-body {
  max-height: 600px;
  opacity: 1;
  margin-top: 1.2rem;
}

.term-detail {
  margin-bottom: 1rem;
}

.term-detail-label {
  font-family: var(--font-mono);
  font-size: 0.65rem;
  font-weight: 600;
  color: var(--accent-green);
  letter-spacing: 0.12em;
  text-transform: uppercase;
  margin-bottom: 0.3rem;
}

.term-detail p {
  font-size: 0.92rem;
  color: var(--text-secondary);
  line-height: 1.65;
}

.term-related {
  display: flex;
  flex-wrap: wrap;
  gap: 0.4rem;
  margin-top: 0.4rem;
}

.term-related a {
  font-family: var(--font-mono);
  font-size: 0.7rem;
  color: var(--accent-cyan);
  background: rgba(0,212,255,0.08);
  border: 1px solid rgba(0,212,255,0.15);
  padding: 0.2rem 0.6rem;
  border-radius: 6px;
  text-decoration: none;
  transition: all 0.2s;
}

.term-related a:hover {
  background: rgba(0,212,255,0.15);
  border-color: rgba(0,212,255,0.3);
}

/* Footer CTA */
.footer-cta {
  max-width: 820px;
  margin: 0 auto;
  padding: 0 2rem 4rem;
}

.footer-cta-box {
  background: linear-gradient(135deg, rgba(0,255,136,0.05), rgba(0,212,255,0.05));
  border: 1px solid var(--border-accent);
  border-radius: 16px;
  padding: 2.5rem;
  text-align: center;
}

.footer-cta-box h3 {
  font-size: 1.2rem;
  font-weight: 600;
  margin-bottom: 0.5rem;
}

.footer-cta-box p {
  color: var(--text-secondary);
  font-size: 0.9rem;
  margin-bottom: 1.5rem;
}

.footer-cta-box .cta-email {
  font-family: var(--font-mono);
  font-size: 0.85rem;
  color: var(--accent-green);
  text-decoration: none;
  border: 1px solid var(--border-accent);
  padding: 0.6rem 1.5rem;
  border-radius: 8px;
  display: inline-block;
  transition: all 0.3s;
}

.footer-cta-box .cta-email:hover {
  background: var(--accent-green-dim);
  box-shadow: 0 0 20px var(--accent-green-dim);
}

/* Footer */
.site-footer {
  border-top: 1px solid var(--border-subtle);
  padding: 2rem;
  text-align: center;
  font-family: var(--font-mono);
  font-size: 0.7rem;
  color: var(--text-muted);
}

.site-footer a { color: var(--accent-green); text-decoration: none; }

/* No results */
.no-results {
  text-align: center;
  padding: 3rem;
  color: var(--text-muted);
  font-size: 0.95rem;
  display: none;
}

.no-results.visible { display: block; }

/* Counter */
.term-count {
  font-family: var(--font-mono);
  font-size: 0.72rem;
  color: var(--text-muted);
  text-align: center;
  margin-bottom: 2rem;
}

.term-count span { color: var(--accent-green); }

/* Responsive */
@media (max-width: 640px) {
  .topbar { padding: 0 1rem; }
  .topbar-nav { gap: 0.8rem; }
  .topbar-nav a { font-size: 0.7rem; }
  .hero { padding: 4rem 1.5rem 2rem; }
  .glossary-container { padding: 0 1rem 4rem; }
  .term-card { padding: 1.3rem 1.5rem; }
  .alpha-nav a { width: 28px; height: 28px; font-size: 0.65rem; }
}

/* Animations */
@keyframes fadeInUp {
  from { opacity: 0; transform: translateY(20px); }
  to { opacity: 1; transform: translateY(0); }
}

.letter-section {
  animation: fadeInUp 0.5s ease forwards;
}
</style>
</head>
<body>

<!-- Top Bar -->
<nav class="topbar">
  <a href="https://hammerlockai.com" class="topbar-logo">HAMMERLOCK<span>AI</span></a>
  <div class="topbar-nav">
    <a href="/blog/blog-index.html">Blog</a>
    <a href="#" class="active">Glossary</a>
    <a href="/blog/citation-library.html">Citations</a>
  </div>
</nav>

<!-- Hero -->
<header class="hero">
  <div class="hero-badge">ðŸ“– Knowledge Base</div>
  <h1>AI Glossary: <em>The Complete Guide</em></h1>
  <p class="hero-sub">Your no-bullshit reference for understanding AI. Plain English definitions, real-world examples, and why each term actually matters.</p>
  <p class="hero-meta">Last Updated: February 20, 2026</p>
</header>

<!-- Search -->
<div class="search-wrap">
  <svg class="search-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><circle cx="11" cy="11" r="8"/><path d="m21 21-4.35-4.35"/></svg>
  <input type="text" id="searchInput" placeholder="Search terms... (e.g. transformer, encryption, RAG)">
</div>

<!-- Alpha Nav -->
<nav class="alpha-nav" id="alphaNav"></nav>

<!-- Term Count -->
<div class="term-count" id="termCount"></div>

<!-- Glossary -->
<main class="glossary-container" id="glossaryContainer"></main>

<!-- No Results -->
<div class="no-results" id="noResults">No terms match your search. Try a different keyword.</div>

<!-- Footer CTA -->
<section class="footer-cta">
  <div class="footer-cta-box">
    <h3>Found an error or missing term?</h3>
    <p>We keep this glossary updated and open source. No corporate jargon. No hype. Just the truth.</p>
    <a href="mailto:research@hammerlockai.com?subject=Glossary Update Request" class="cta-email">research@hammerlockai.com</a>
  </div>
</section>

<!-- Footer -->
<footer class="site-footer">
  Â© 2026 <a href="https://hammerlockai.com">HammerLockAI</a> â€” Your AI. Your Data. Your Rules.<br>
  Maintained as open source. Contributions welcome.
</footer>

<script>
const glossaryData = [
  {
    letter: "A",
    terms: [
      {
        title: "AGI (Artificial General Intelligence)",
        abbr: "AGI",
        definition: "AI that can understand, learn, and apply knowledge across any intellectual task that a human can perform. Not specializedâ€”general purpose.",
        matters: "We don't have it yet. All current AI is \"narrow\" (good at specific tasks). AGI is the holy grail that everyone's chasing and nobody's achieved.",
        example: "ChatGPT is NOT AGIâ€”it's very good at language but can't drive a car, diagnose diseases, or write symphonies without being specifically trained. True AGI would do all of these without specialized training.",
        related: ["Narrow AI", "ASI (Artificial Super Intelligence)"]
      },
      {
        title: "AES-256 (Advanced Encryption Standard, 256-bit)",
        abbr: "AES-256",
        definition: "A symmetric encryption algorithm using 256-bit keys. The current gold standard for data security.",
        matters: "It would take all the computers on Earth billions of years to brute-force crack AES-256. When your data is \"AES-256 encrypted,\" it's effectively unbreakable with current technology.",
        example: "Your banking app, Signal messages, and HammerLockAI's local storage all use AES-256 to keep your data private.",
        related: ["Encryption", "End-to-End Encryption", "Zero-Knowledge"]
      },
      {
        title: "Agent (AI Agent)",
        abbr: null,
        definition: "An AI system that can perceive its environment, make decisions, and take actions autonomously to achieve specific goals.",
        matters: "Unlike chatbots that just respond, agents can actually DO thingsâ€”schedule meetings, write code, search the web, execute commands.",
        example: "OpenClaw, AutoGPT, and similar systems that can break down complex tasks and execute them step-by-step without constant human input.",
        related: ["Agentic AI", "Autonomous Systems", "Multi-Agent Systems"]
      },
      {
        title: "AlphaGo",
        abbr: null,
        definition: "DeepMind's AI that mastered the game of Go using deep reinforcement learning and neural networks.",
        matters: "In 2016, it beat Lee Sedol (world champion) 4-1. Go is exponentially more complex than chessâ€”this was thought impossible for decades.",
        example: "AlphaGo played \"Move 37\" in game 2â€”a move so unusual it had a 1 in 10,000 chance of being played by humans. It won the game and changed how professional Go players think.",
        related: ["Deep Learning", "Reinforcement Learning", "DeepMind"]
      },
      {
        title: "API (Application Programming Interface)",
        abbr: "API",
        definition: "A set of rules that allows different software applications to communicate with each other.",
        matters: "How AI services (OpenAI, Anthropic, Google) let you access their models programmatically. You send a request, they send back a response.",
        example: "When HammerLockAI sends your query to Claude or GPT-4, it's using their API. Same when it searches Braveâ€”API call.",
        related: ["API Key", "REST API", "Rate Limiting"]
      },
      {
        title: "Attention Mechanism",
        abbr: null,
        definition: "A technique that allows AI models to focus on the most relevant parts of input data when making predictions.",
        matters: "The breakthrough that made transformers possible. Instead of processing everything equally, the model learns what matters most.",
        example: "When translating \"The bank closed at 3PM\" vs \"I sat by the river bank,\" attention helps the model understand which meaning of \"bank\" is relevant based on context.",
        related: ["Transformer", "Self-Attention", "Multi-Head Attention"]
      }
    ]
  },
  {
    letter: "B",
    terms: [
      {
        title: "BERT (Bidirectional Encoder Representations from Transformers)",
        abbr: "BERT",
        definition: "Google's transformer-based model that reads text bidirectionally (looks at words before AND after) to understand context.",
        matters: "Revolutionized search and NLP by understanding context better than previous models that only read left-to-right.",
        example: "When you Google \"2023 Denver Broncos quarterback,\" BERT understands you want current information, not historical data, because of temporal context clues.",
        related: ["Transformer", "NLP", "Pre-training"]
      },
      {
        title: "Bias (AI Bias)",
        abbr: null,
        definition: "Systematic errors in AI outputs that reflect prejudices in training data or design choices.",
        matters: "AI models learn from data. If the data reflects human biases (racism, sexism, etc.), the model reproduces them. This is a serious problem.",
        example: "Early facial recognition systems had higher error rates for darker-skinned faces because training data was mostly light-skinned faces.",
        related: ["Training Data", "Dataset Bias", "Fairness"]
      },
      {
        title: "BYOK (Bring Your Own Key)",
        abbr: "BYOK",
        definition: "Using your own API keys from AI providers instead of going through a middleman service.",
        matters: "You pay the provider directly at their rates (no markup), have full control over usage, and your queries don't route through third parties.",
        example: "HammerLockAI lets you use your own OpenAI, Anthropic, or Google API keys instead of requiring you to use theirs.",
        related: ["API Key", "Zero Markup", "Direct Integration"]
      }
    ]
  },
  {
    letter: "C",
    terms: [
      {
        title: "ChatGPT",
        abbr: null,
        definition: "OpenAI's conversational AI based on the GPT (Generative Pre-trained Transformer) architecture.",
        matters: "The first LLM that went truly mainstream (100M users in 2 months). Launched the current AI boom.",
        example: "You're using it when you ask ChatGPT to write code, summarize documents, or explain concepts. It's a chatbot interface to GPT models.",
        related: ["GPT", "LLM", "Generative AI"]
      },
      {
        title: "Claude",
        abbr: null,
        definition: "Anthropic's family of large language models, named after Claude Shannon (information theory pioneer).",
        matters: "Known for longer context windows, strong reasoning, and \"Constitutional AI\" training methods that emphasize helpfulness and harmlessness.",
        example: "Claude Sonnet 4.5 can process 200,000+ tokensâ€”enough to read entire books in one prompt. Used by many for complex analysis and long-form writing.",
        related: ["LLM", "Anthropic", "Constitutional AI"]
      },
      {
        title: "Context Window",
        abbr: null,
        definition: "The maximum amount of text (measured in tokens) that an AI model can process at once.",
        matters: "Determines how much information the model can \"remember\" within a single conversation or task. Larger = better for complex work.",
        example: "GPT-4 has a 128K token context window (~96,000 words). Claude Sonnet 4.5 has 200K+ tokens. Earlier models had only 4K-8K.",
        related: ["Token", "Memory", "Input Length"]
      },
      {
        title: "CRDT (Conflict-free Replicated Data Type)",
        abbr: "CRDT",
        definition: "A data structure that allows multiple users to edit the same data simultaneously and merge changes without conflicts.",
        matters: "Enables real-time collaboration without a central server. Each user can work offline and changes merge correctly when syncing.",
        example: "Google Docs uses CRDT-like technology so multiple people can type at once without losing anyone's work. Essential for local-first apps.",
        related: ["Local-First", "Distributed Systems", "Sync"]
      }
    ]
  },
  {
    letter: "D",
    terms: [
      {
        title: "Deep Learning",
        abbr: null,
        definition: "Machine learning using neural networks with many layers (\"deep\"). Enables learning complex patterns from massive data.",
        matters: "Powers almost all modern AI breakthroughsâ€”image recognition, language models, game playing, drug discovery.",
        example: "AlphaFold uses deep learning to predict protein structures. GPT uses deep learning for language. Tesla uses it for self-driving. It's everywhere.",
        related: ["Neural Network", "Machine Learning", "Backpropagation"]
      },
      {
        title: "Diffusion Model",
        abbr: null,
        definition: "AI model that generates content by starting with noise and gradually refining it into a coherent output.",
        matters: "Powers the AI image revolutionâ€”DALL-E, Midjourney, Stable Diffusion all use this approach.",
        example: "When Midjourney creates an image from your text prompt, it starts with random noise and iteratively denoises it into a photorealistic image.",
        related: ["Generative AI", "Stable Diffusion", "GAN"]
      }
    ]
  },
  {
    letter: "E",
    terms: [
      {
        title: "Embeddings",
        abbr: null,
        definition: "Numerical representations of text (or images, audio) where similar concepts are closer together in mathematical space.",
        matters: "How AI understands \"meaning.\" Similar words/concepts become similar numbers, enabling semantic search and RAG.",
        example: "\"King\" minus \"man\" plus \"woman\" â‰ˆ \"queen\" in embedding space. The math literally captures semantic relationships.",
        related: ["Vector Database", "Semantic Search", "RAG"]
      },
      {
        title: "End-to-End Encryption (E2EE)",
        abbr: "E2EE",
        definition: "Encryption where only the sender and recipient can read messages. No middleman (not even the service provider) has access.",
        matters: "The only truly private communication. If a service is E2EE, even if their servers are hacked, your messages remain encrypted.",
        example: "Signal, WhatsApp (messages only), and HammerLockAI all use E2EE. iMessages between Apple devices are also E2EE.",
        related: ["AES-256", "Zero-Knowledge", "Privacy"]
      },
      {
    letter: "F",
    terms: [
      {
        title: "Few-Shot Learning",
        abbr: null,
        definition: "AI's ability to learn a task from just a few examples provided in the prompt, without retraining.",
        matters: "Why LLMs are so flexible. Show it 2-3 examples of what you want and it generalizes to new cases.",
        example: "Give GPT-4 three examples of converting addresses to coordinates, then ask it to do a fourth. It understands the pattern without being retrained.",
        related: ["Zero-Shot Learning", "Prompt Engineering", "In-Context Learning"]
      },
      {
        title: "Fine-Tuning",
        abbr: null,
        definition: "Taking a pre-trained AI model and training it further on specialized data for a specific task or domain.",
        matters: "How you customize a general model for your needs. Much cheaper and faster than training from scratch.",
        example: "A hospital takes LLaMA 3.1 and fine-tunes it on medical records to create a model that excels at clinical summarization.",
        related: ["Pre-Training", "Transfer Learning", "LoRA"]
      },
      {
        title: "Foundation Model",
        abbr: null,
        definition: "A large AI model trained on broad data that can be adapted to many tasks. The \"foundation\" other applications build on.",
        matters: "GPT-4, Claude, LLaMAâ€”these are all foundation models. One training run, infinite applications.",
        example: "LLaMA 3.1 is a foundation model. From it, people build coding assistants, chatbots, medical tools, legal analyzersâ€”all without training from zero.",
        related: ["Pre-Training", "Fine-Tuning", "Transfer Learning"]
      }
    ]
  },
  {
    letter: "G",
    terms: [
      {
        title: "GAN (Generative Adversarial Network)",
        abbr: "GAN",
        definition: "Two neural networks competing: a generator creates fake content, a discriminator tries to detect fakes. Both improve through competition.",
        matters: "Pioneered AI-generated images before diffusion models. Still used for specific applications like data augmentation.",
        example: "ThisPersonDoesNotExist.com uses GANs to generate photorealistic faces of people who never existed.",
        related: ["Generative AI", "Diffusion Model", "Neural Network"]
      },
      {
        title: "GPT (Generative Pre-trained Transformer)",
        abbr: "GPT",
        definition: "OpenAI's series of large language models. GPT-3, GPT-4, GPT-4oâ€”each generation more capable.",
        matters: "The most commercially successful LLM family. Set the standard for what AI assistants can do.",
        example: "GPT-4 can write code, analyze images, pass the bar exam, and engage in complex reasoning. It's the model behind ChatGPT.",
        related: ["Transformer", "LLM", "ChatGPT"]
      }
    ]
  },
  {
    letter: "H",
    terms: [
      {
        title: "Hallucination",
        abbr: null,
        definition: "When an AI model generates information that sounds confident and plausible but is factually wrong or completely made up.",
        matters: "The biggest trust problem in AI. Models don't \"know\" thingsâ€”they predict likely text. Sometimes that prediction is wrong.",
        example: "Ask ChatGPT for legal citations and it might invent case names that don't exist. The text reads perfectlyâ€”but the cases are fictional.",
        related: ["RAG", "Grounding", "Factuality"]
      },
      {
        title: "Hugging Face",
        abbr: null,
        definition: "The largest open platform for sharing AI models, datasets, and tools. The \"GitHub of machine learning.\"",
        matters: "Democratized AI by making models accessible. 500K+ models available for free download and use.",
        example: "Want to run LLaMA locally? Download it from Hugging Face. Need a sentiment analysis model? Hugging Face. Fine-tuned model for your language? Hugging Face.",
        related: ["Open Source", "Model Hub", "Transformers Library"]
      }
    ]
  },
  {
    letter: "L",
    terms: [
      {
        title: "LLM (Large Language Model)",
        abbr: "LLM",
        definition: "AI model trained on massive text data that can understand, generate, and reason about language.",
        matters: "The technology behind ChatGPT, Claude, Gemini, and the entire current AI revolution. The most commercially impactful AI development.",
        example: "GPT-4, Claude Sonnet, LLaMA 3.1, Mistralâ€”all LLMs. They're trained on billions of words and learn to predict what comes next.",
        related: ["Transformer", "Foundation Model", "GPT"]
      },
      {
        title: "Local AI (On-Device AI)",
        abbr: null,
        definition: "Running AI models directly on your own hardware instead of sending data to cloud servers.",
        matters: "Your data never leaves your machine. No internet required. No monthly API fees. Complete privacy and control.",
        example: "Running LLaMA 3.1 through Ollama on your laptop. The model runs locallyâ€”your prompts never touch anyone's server.",
        related: ["Ollama", "Privacy", "Edge Computing"]
      },
      {
        title: "LoRA (Low-Rank Adaptation)",
        abbr: "LoRA",
        definition: "Efficient fine-tuning technique that trains only small adapter layers instead of the entire model.",
        matters: "Makes fine-tuning accessible. Instead of needing massive GPU clusters, you can customize a 70B model on a single GPU.",
        example: "Fine-tuning full LLaMA 3.1 70B requires 140GB+ VRAM. With LoRA, you can do it on a 24GB GPU by only training ~1% of parameters.",
        related: ["Fine-Tuning", "QLoRA", "Parameter-Efficient Training"]
      }
    ]
  },
  {
    letter: "M",
    terms: [
      {
        title: "Machine Learning (ML)",
        abbr: "ML",
        definition: "Subset of AI where systems learn from data without being explicitly programmed for every scenario.",
        matters: "The fundamental technique behind modern AI. Instead of writing rules, you show the system examples and it learns patterns.",
        example: "Spam filters learn from millions of emails what spam looks like. No one programs every spam patternâ€”the model learns them.",
        related: ["Deep Learning", "Neural Network", "Training Data"]
      },
      {
        title: "Multi-Agent System",
        abbr: null,
        definition: "Multiple AI agents working together, each specialized in different tasks, coordinating to achieve complex goals.",
        matters: "Single agents hit limits. Multiple specialized agentsâ€”one for research, one for coding, one for reviewâ€”produce better results.",
        example: "OpenClaw uses 15 specialized agents (security, DevOps, marketing, UI/UX) that collaborate autonomously on development tasks.",
        related: ["Agent", "Agentic AI", "Orchestration"]
      }
    ]
  },
  {
    letter: "N",
    terms: [
      {
        title: "Neural Network",
        abbr: null,
        definition: "Computing system inspired by biological brains. Layers of interconnected \"neurons\" that learn patterns from data.",
        matters: "The architecture behind all modern AI. Simple concept, profound results when scaled up.",
        example: "A single neuron: takes inputs, applies weights, produces output. Stack millions in layers and you get GPT-4, DALL-E, AlphaFold.",
        related: ["Deep Learning", "Machine Learning", "Transformer"]
      },
      {
        title: "NLP (Natural Language Processing)",
        abbr: "NLP",
        definition: "Branch of AI that deals with understanding, interpreting, and generating human language.",
        matters: "Every chatbot, translator, search engine, and voice assistant uses NLP. It's AI's interface to human communication.",
        example: "When Siri understands \"set an alarm for 7AM\" or Google translates between languagesâ€”that's NLP.",
        related: ["LLM", "BERT", "Transformer"]
      }
    ]
  },
  {
    letter: "O",
    terms: [
      {
        title: "Ollama",
        abbr: null,
        definition: "Open-source tool for running LLMs locally on your own machine. Makes local AI as easy as a single terminal command.",
        matters: "Removes the complexity from local AI. One command to download and run any supported model. No PhD required.",
        example: "Type `ollama run llama3.1` and you have a 70B parameter model running locally. No API key, no subscription, no data leaving your machine.",
        related: ["Local AI", "LLaMA", "Open Source"]
      },
      {
        title: "Open Source AI",
        abbr: null,
        definition: "AI models and tools whose code, weights, and training methodology are publicly available for anyone to use, modify, and redistribute.",
        matters: "Democratizes AI. No single company controls the technology. Anyone can inspect, improve, and customize these models.",
        example: "LLaMA (Meta), Mistral, Gemma (Google), DeepSeekâ€”all open source. You can download and run them with zero restrictions.",
        related: ["LLaMA", "Mistral", "Hugging Face"]
      }
    ]
  },
  {
    letter: "P",
    terms: [
      {
        title: "Parameter",
        abbr: null,
        definition: "A learnable value in a neural network. More parameters generally = more capable model (with diminishing returns).",
        matters: "How we measure model size. GPT-4 has ~1.8 trillion parameters. LLaMA 3.1's largest is 405 billion. Size matters, but efficiency matters more.",
        example: "A 7B model runs on a laptop. A 70B model needs a beefy workstation. A 405B model needs server-grade hardware. Parameters determine hardware requirements.",
        related: ["Neural Network", "Training", "Model Size"]
      },
      {
        title: "Pre-Training",
        abbr: null,
        definition: "The initial, massive training phase where a model learns from enormous datasets (often most of the internet).",
        matters: "The expensive part. Pre-training GPT-4 cost ~$100M+. This is where the model learns language, facts, and reasoning patterns.",
        example: "GPT-4 was pre-trained on most of the internet. Then fine-tuned for helpfulness. When you use it, you benefit from both phases.",
        related: ["Fine-Tuning", "Transfer Learning", "Foundation Model"]
      },
      {
        title: "Prompt Engineering",
        abbr: null,
        definition: "The art of crafting inputs (prompts) to AI models to get desired outputs. Surprisingly important for getting good results.",
        matters: "Same model, different prompt = vastly different quality. Good prompting is a skill that dramatically improves AI usefulness.",
        example: "\"Write a blog post\" vs \"Write a 1000-word blog post about local AI, targeting technical readers, with a skeptical but informative tone, including pros/cons.\" Guess which gets better results?",
        related: ["Few-Shot Learning", "System Prompt", "Chain-of-Thought"]
      }
    ]
  },
  {
    letter: "R",
    terms: [
      {
        title: "RAG (Retrieval-Augmented Generation)",
        abbr: "RAG",
        definition: "Technique that enhances LLM responses by first retrieving relevant information from a database, then using it to generate answers.",
        matters: "Reduces hallucinations, enables citations, and lets models access knowledge beyond their training data.",
        example: "Instead of GPT-4 answering from memory (risking hallucination), RAG searches your company docs, retrieves relevant passages, then generates an answer based on those.",
        related: ["Vector Database", "Semantic Search", "Embeddings"]
      },
      {
        title: "Reinforcement Learning (RL)",
        abbr: "RL",
        definition: "Machine learning where an agent learns by trial and error, receiving rewards for good actions and penalties for bad ones.",
        matters: "How AlphaGo learned Go, how ChatGPT learned to follow instructions (RLHF), how robots learn to walk.",
        example: "AlphaGo Zero learned entirely through self-playâ€”no human games. Played millions of games against itself, learning from wins/losses.",
        related: ["AlphaGo", "RLHF", "Deep Reinforcement Learning"]
      },
      {
        title: "RLHF (Reinforcement Learning from Human Feedback)",
        abbr: "RLHF",
        definition: "Training technique where humans rate AI outputs, and the model learns to produce responses humans prefer.",
        matters: "How ChatGPT became helpful rather than just predictive. Transformed LLMs from \"complete this text\" to \"answer this question helpfully.\"",
        example: "Early GPT-3 might complete \"The president should...\" with news-style text. RLHF-trained GPT-3.5 recognizes it as a question and provides a thoughtful response.",
        related: ["Alignment", "Constitutional AI", "Human Feedback"]
      }
    ]
  },
  {
    letter: "S",
    terms: [
      {
        title: "Semantic Search",
        abbr: null,
        definition: "Search based on meaning and context, not just keyword matching.",
        matters: "Traditional search finds \"cat\" when you search \"cat.\" Semantic search finds \"feline,\" \"kitten,\" \"tabby\"â€”anything conceptually related.",
        example: "Search \"flights to the Big Apple\" and semantic search understands you mean New York, not apple orchards.",
        related: ["Embeddings", "Vector Database", "RAG"]
      },
      {
        title: "Self-Attention",
        abbr: null,
        definition: "Mechanism in transformers where each word in a sequence learns to pay attention to other relevant words.",
        matters: "The core innovation that made transformers superior to previous architectures. Enables understanding long-range dependencies.",
        example: "In \"The animal didn't cross the street because it was too tired,\" self-attention helps the model understand \"it\" refers to the animal, not the street.",
        related: ["Attention Mechanism", "Transformer", "BERT"]
      }
    ]
  },
  {
    letter: "T",
    terms: [
      {
        title: "Token",
        abbr: null,
        definition: "The basic unit of text that LLMs process. Roughly 0.75 words in English. \"Hello world\" = 2 tokens.",
        matters: "API pricing is per token. Context windows are measured in tokens. Understanding tokens = understanding costs and limits.",
        example: "1,000 tokens â‰ˆ 750 words. GPT-4 with 128K context window can process ~96,000 words at once. Costs vary by model and provider.",
        related: ["Tokenization", "Context Window", "Input/Output Tokens"]
      },
      {
        title: "Transformer",
        abbr: null,
        definition: "Neural network architecture that revolutionized NLP. Uses attention mechanisms instead of sequential processing.",
        matters: "The \"T\" in GPT, BERT, and most modern LLMs. Enabled models to understand context across long sequences efficiently.",
        example: "Published in 2017 paper \"Attention Is All You Need.\" Within 5 years, transformers dominated AI. They're why we have ChatGPT.",
        related: ["Attention Mechanism", "BERT", "GPT", "Self-Attention"]
      },
      {
        title: "Turing Test",
        abbr: null,
        definition: "Proposed by Alan Turing in 1950. If a human can't distinguish machine responses from human responses, the machine demonstrates intelligence.",
        matters: "Not a perfect test (you can fool humans without being intelligent), but established the principle that AI should be judged by what it does, not how it's built.",
        example: "Modern LLMs can arguably pass the Turing Test in many contexts. But they're not \"generally intelligent\"â€”just very good at language.",
        related: ["Alan Turing", "AGI", "Imitation Game"]
      }
    ]
  },
  {
    letter: "V",
    terms: [
      {
        title: "Vector Database",
        abbr: null,
        definition: "Database optimized for storing and searching embeddings (high-dimensional vectors) using similarity/distance measures.",
        matters: "Enables semantic search and RAG. Essential infrastructure for AI applications that need to search knowledge bases by meaning.",
        example: "Pinecone, Weaviate, ChromaDB. When you ask \"find documents about cost savings,\" vector DB finds conceptually similar content even with different words.",
        related: ["Embeddings", "RAG", "Semantic Search"]
      }
    ]
  },
  {
    letter: "W",
    terms: [
      {
        title: "Watson (IBM Watson)",
        abbr: null,
        definition: "IBM's AI system that famously won Jeopardy! in 2011, beating champions Ken Jennings and Brad Rutter.",
        matters: "Demonstrated AI's ability to understand natural language questions and retrieve precise answers from vast knowledge bases.",
        example: "Could parse complex Jeopardy! clues with wordplay and double meanings, then buzz in with correct responses faster than human champions.",
        related: ["Question Answering", "NLP", "IBM"]
      }
    ]
  },
  {
    letter: "Z",
    terms: [
      {
        title: "Zero-Knowledge",
        abbr: null,
        definition: "Cryptographic approach where a service can verify something is true without learning what that something is.",
        matters: "Ultimate privacy. The service doesn't see your data, even while processing it or verifying it.",
        example: "Signal uses zero-knowledge architectureâ€”they can't read your messages even if subpoenaed because they never have the decryption keys.",
        related: ["End-to-End Encryption", "Privacy", "Cryptography"]
      },
      {
        title: "Zero-Shot Learning",
        abbr: null,
        definition: "AI's ability to perform tasks it wasn't explicitly trained on, using only its general knowledge and the task description.",
        matters: "Shows models understand concepts, not just patterns. Can generalize to novel situations.",
        example: "Ask GPT-4 to translate English to Klingon (which it barely saw in training). It can do it by understanding translation as a concept and applying linguistic patterns.",
        related: ["Few-Shot Learning", "Transfer Learning", "Generalization"]
      }
    ]
  }
];

// Build alphabet nav
const alphaNav = document.getElementById('alphaNav');
const letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'.split('');
const activeLetters = glossaryData.map(s => s.letter);

letters.forEach(l => {
  const a = document.createElement('a');
  a.href = `#section-${l}`;
  a.textContent = l;
  if (activeLetters.includes(l)) a.classList.add('has-entries');
  alphaNav.appendChild(a);
});

// Build glossary
const container = document.getElementById('glossaryContainer');
let totalTerms = 0;

glossaryData.forEach(section => {
  const sectionEl = document.createElement('div');
  sectionEl.className = 'letter-section';
  sectionEl.id = `section-${section.letter}`;

  const heading = document.createElement('div');
  heading.className = 'letter-heading';
  heading.textContent = `Section ${section.letter}`;
  sectionEl.appendChild(heading);

  section.terms.forEach(term => {
    totalTerms++;
    const card = document.createElement('div');
    card.className = 'term-card';
    card.dataset.search = (term.title + ' ' + term.definition + ' ' + term.matters + ' ' + (term.abbr || '')).toLowerCase();

    const titleRow = document.createElement('div');
    titleRow.className = 'term-title';
    
    const titleText = document.createElement('span');
    titleText.innerHTML = term.title + (term.abbr ? ` <span class="abbr">${term.abbr}</span>` : '');
    
    const expandIcon = document.createElementNS('http://www.w3.org/2000/svg', 'svg');
    expandIcon.classList.add('term-expand-icon');
    expandIcon.setAttribute('viewBox', '0 0 24 24');
    expandIcon.setAttribute('fill', 'none');
    expandIcon.setAttribute('stroke', 'currentColor');
    expandIcon.setAttribute('stroke-width', '2');
    expandIcon.innerHTML = '<polyline points="6 9 12 15 18 9"/>';

    titleRow.appendChild(titleText);
    titleRow.appendChild(expandIcon);
    card.appendChild(titleRow);

    const body = document.createElement('div');
    body.className = 'term-body';

    const details = [
      { label: 'What it is', text: term.definition },
      { label: 'Why it matters', text: term.matters },
      { label: 'Real example', text: term.example }
    ];

    details.forEach(d => {
      const detail = document.createElement('div');
      detail.className = 'term-detail';
      detail.innerHTML = `<div class="term-detail-label">${d.label}</div><p>${d.text}</p>`;
      body.appendChild(detail);
    });

    if (term.related && term.related.length) {
      const relDiv = document.createElement('div');
      relDiv.className = 'term-detail';
      relDiv.innerHTML = '<div class="term-detail-label">Related terms</div>';
      const relLinks = document.createElement('div');
      relLinks.className = 'term-related';
      term.related.forEach(r => {
        const a = document.createElement('a');
        a.href = '#';
        a.textContent = r;
        a.addEventListener('click', (e) => {
          e.preventDefault();
          document.getElementById('searchInput').value = r;
          document.getElementById('searchInput').dispatchEvent(new Event('input'));
        });
        relLinks.appendChild(a);
      });
      relDiv.appendChild(relLinks);
      body.appendChild(relDiv);
    }

    card.appendChild(body);
    card.addEventListener('click', () => card.classList.toggle('expanded'));
    sectionEl.appendChild(card);
  });

  container.appendChild(sectionEl);
});

document.getElementById('termCount').innerHTML = `<span>${totalTerms}</span> terms defined`;

// Search
const searchInput = document.getElementById('searchInput');
const noResults = document.getElementById('noResults');

searchInput.addEventListener('input', () => {
  const query = searchInput.value.toLowerCase().trim();
  const cards = document.querySelectorAll('.term-card');
  const sections = document.querySelectorAll('.letter-section');
  let visibleCount = 0;

  cards.forEach(card => {
    const match = !query || card.dataset.search.includes(query);
    card.style.display = match ? '' : 'none';
    if (match) visibleCount++;
  });

  sections.forEach(section => {
    const visibleCards = section.querySelectorAll('.term-card:not([style*="display: none"])');
    section.style.display = visibleCards.length ? '' : 'none';
  });

  noResults.classList.toggle('visible', visibleCount === 0 && query);
  document.getElementById('termCount').innerHTML = query
    ? `<span>${visibleCount}</span> of ${totalTerms} terms`
    : `<span>${totalTerms}</span> terms defined`;
});
</script>
</body>
</html>
