import { NextResponse } from "next/server";
import { exec } from "child_process";
import { promisify } from "util";
import path from "path";
import os from "os";
import fs from "fs/promises";
import { hasCredit, deductCredit, getRemainingUnits } from "@/lib/compute-credits";
import { createAnonymizer } from "@/lib/anonymize";
import { config as dotenvConfig } from "dotenv";
import { decryptFromFile, encryptForFile, hasServerSessionKey, isEncrypted } from "@/lib/server-crypto";

// Load user env from ~/.hammerlock/.env (for Electron packaged builds)
// Note: If .env is encrypted, keys are loaded on vault unlock via /api/vault-session
try {
  const envPath = path.join(os.homedir(), ".hammerlock", ".env");
  const envRaw = require("fs").readFileSync(envPath, "utf8");
  if (!isEncrypted(envRaw)) {
    // Only load via dotenv if file is plaintext (pre-encryption migration)
    dotenvConfig({ path: envPath });
  }
} catch { /* .env doesn't exist yet â€” that's fine */ }

// Allow longer execution for LLM calls on Vercel (default 10s is too short)
export const maxDuration = 30;

const execAsync = promisify(exec);
const personaPath = path.join(os.homedir(), ".hammerlock", "persona.md");
const planPath = path.join(os.homedir(), ".hammerlock", "plan.md");
const vaultJsonPath = path.join(process.cwd(), "vault.json");

// Read at runtime to pick up keys configured after module init
function getBraveKey() { return process.env.BRAVE_API_KEY || ""; }
const OLLAMA_BASE_URL = process.env.OLLAMA_BASE_URL || "http://localhost:11434";
const OLLAMA_MODEL = process.env.OLLAMA_MODEL || "phi3";



let cachedPersona = "";

async function readFileSafe(target: string) {
  const raw = await fs.readFile(target, "utf8");
  // Decrypt if the file is encrypted and we have a session key
  if (isEncrypted(raw)) {
    const decrypted = decryptFromFile(raw);
    if (decrypted === null) throw new Error("File is encrypted but vault is locked. Unlock your vault first.");
    return decrypted;
  }
  return raw;
}

async function runStatus() {
  const vaultExists = await fs
    .access(vaultJsonPath)
    .then(() => true)
    .catch(() => false);
  const personaExists = await fs
    .access(personaPath)
    .then(() => true)
    .catch(() => false);

  const ollamaUp = await callOllama("test", "ping").then(() => true).catch(() => false);
  const hasOpenAI = !!process.env.OPENAI_API_KEY;
  const hasAnthropic = !!process.env.ANTHROPIC_API_KEY;
  const hasGemini = !!process.env.GEMINI_API_KEY;
  const hasGroq = !!process.env.GROQ_API_KEY;
  const hasMistral = !!process.env.MISTRAL_API_KEY;
  const hasDeepSeek = !!process.env.DEEPSEEK_API_KEY;
  const hasBrave = !!getBraveKey();

  const lines = [
    `**HammerLock AI Status**`,
    ``,
    `Vault: ${vaultExists ? "active" : "not set up"}`,
    `Persona: ${personaExists ? "loaded" : "not found"} (${personaPath})`,
    ``,
    `**Providers**`,
    `Ollama: ${ollamaUp ? "connected" : "offline"}`,
    `OpenAI: ${hasOpenAI ? "configured" : "not set"}`,
    `Anthropic: ${hasAnthropic ? "configured" : "not set"}`,
    `Gemini: ${hasGemini ? "configured" : "not set"}`,
    `Groq: ${hasGroq ? "configured" : "not set"}`,
    `Mistral: ${hasMistral ? "configured" : "not set"}`,
    `DeepSeek: ${hasDeepSeek ? "configured" : "not set"}`,
    `Brave Search: ${hasBrave ? "configured" : "not set"}`,
  ];
  return lines.join("\n");
}

const SAFE_BASE_DIR = path.join(os.homedir(), ".hammerlock");

function sanitizePath(raw: string): string {
  const trimmed = raw.replace(/^['" ]+|['" ]+$/g, "");
  // Resolve relative paths against the safe directory
  const resolved = path.isAbsolute(trimmed)
    ? path.resolve(trimmed)
    : path.resolve(SAFE_BASE_DIR, trimmed);
  // Block path traversal â€” resolved path must stay inside ~/.hammerlock/
  if (!resolved.startsWith(SAFE_BASE_DIR + path.sep) && resolved !== SAFE_BASE_DIR) {
    throw new Error("Access denied: file reads are restricted to ~/.hammerlock/");
  }
  return resolved;
}


async function loadPersonaText() {
  if (cachedPersona) return cachedPersona;
  try {
    const raw = await fs.readFile(personaPath, "utf8");
    // Decrypt if encrypted
    if (isEncrypted(raw)) {
      const decrypted = decryptFromFile(raw);
      if (decrypted) {
        cachedPersona = decrypted;
        return cachedPersona;
      }
      // Encrypted but no session key â€” can't read persona
      return "";
    }
    cachedPersona = raw;
    return cachedPersona;
  } catch {
    try {
      const raw = await fs.readFile(vaultJsonPath, "utf8");
      const parsed = JSON.parse(raw);
      const profile = parsed?.profile;
      if (profile) {
        cachedPersona = `Name: ${profile.name || ""}\nRole: ${profile.role || ""}\nLocation: ${profile.location || ""}`;
        return cachedPersona;
      }
    } catch {
      /* ignore */
    }
  }
  return "";
}

/** Read current persona text (decrypting if needed), append a line, write back encrypted. */
async function appendToPersona(line: string): Promise<string> {
  let existing = "";
  try {
    const raw = await fs.readFile(personaPath, "utf8");
    if (isEncrypted(raw)) {
      existing = decryptFromFile(raw) || "";
    } else {
      existing = raw;
    }
  } catch { /* new file */ }
  const updated = existing ? `${existing}\n${line}` : line;
  await fs.mkdir(path.dirname(personaPath), { recursive: true });
  const toWrite = hasServerSessionKey() ? encryptForFile(updated) : updated;
  await fs.writeFile(personaPath, toWrite, "utf8");
  cachedPersona = updated;
  return updated;
}

// â”€â”€ Open-Meteo Weather API (free, no key needed) â”€â”€
// WMO weather codes â†’ human-readable conditions
const WMO_CODES: Record<number, string> = {
  0: "Clear sky", 1: "Mainly clear", 2: "Partly cloudy", 3: "Overcast",
  45: "Foggy", 48: "Icy fog", 51: "Light drizzle", 53: "Moderate drizzle",
  55: "Dense drizzle", 61: "Light rain", 63: "Moderate rain", 65: "Heavy rain",
  66: "Light freezing rain", 67: "Heavy freezing rain",
  71: "Light snow", 73: "Moderate snow", 75: "Heavy snow", 77: "Snow grains",
  80: "Light showers", 81: "Moderate showers", 82: "Violent showers",
  85: "Light snow showers", 86: "Heavy snow showers",
  95: "Thunderstorm", 96: "Thunderstorm with light hail", 99: "Thunderstorm with heavy hail",
};

// Simple city â†’ lat/lon lookup for common US cities (extend as needed)
const CITY_COORDS: Record<string, [number, number]> = {
  "san ramon": [37.7799, -121.978],
  "san francisco": [37.7749, -122.4194],
  "los angeles": [34.0522, -118.2437],
  "new york": [40.7128, -74.0060],
  "seattle": [47.6062, -122.3321],
  "austin": [30.2672, -97.7431],
  "chicago": [41.8781, -87.6298],
  "denver": [39.7392, -104.9903],
  "miami": [25.7617, -80.1918],
  "portland": [45.5152, -122.6784],
  "san jose": [37.3382, -121.8863],
  "oakland": [37.8044, -122.2712],
  "sacramento": [38.5816, -121.4944],
  "san diego": [32.7157, -117.1611],
  "dublin": [37.7159, -121.9358],
  "pleasanton": [37.6604, -121.8758],
  "walnut creek": [37.9101, -122.0652],
  "danville": [37.8216, -121.9999],
};

function getCoordsForLocation(location: string | null): [number, number] | null {
  if (!location) return null;
  const lower = location.toLowerCase().replace(/,?\s*(ca|tx|ny|fl|wa|il|az|or|co|ga|ma)\b.*$/i, "").trim();
  return CITY_COORDS[lower] || null;
}

async function fetchWeatherData(lat: number, lon: number): Promise<string | null> {
  try {
    const controller = new AbortController();
    const timeout = setTimeout(() => controller.abort(), 5000);
    const url = `https://api.open-meteo.com/v1/forecast?latitude=${lat}&longitude=${lon}&current=temperature_2m,apparent_temperature,rain,weather_code,wind_speed_10m&daily=weather_code,temperature_2m_max,temperature_2m_min,precipitation_probability_max&temperature_unit=fahrenheit&wind_speed_unit=mph&forecast_days=7&timezone=America/Los_Angeles`;
    const res = await fetch(url, { signal: controller.signal });
    clearTimeout(timeout);
    if (!res.ok) return null;
    const data = await res.json();

    const c = data.current;
    const d = data.daily;
    const condition = WMO_CODES[c.weather_code] || "Unknown";
    const days = ["Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"];

    let forecast = `REAL-TIME WEATHER DATA (from Open-Meteo, accurate to this moment):\n`;
    forecast += `Current: ${Math.round(c.temperature_2m)}Â°F, feels like ${Math.round(c.apparent_temperature)}Â°F, ${condition}`;
    forecast += c.rain > 0 ? `, rain: ${c.rain}mm` : "";
    forecast += `, wind: ${Math.round(c.wind_speed_10m)} mph\n\n`;
    forecast += `7-Day Forecast:\n`;

    for (let i = 0; i < d.time.length; i++) {
      const date = new Date(d.time[i] + "T12:00");
      const dayName = days[date.getDay()];
      const dateLabel = date.toLocaleDateString("en-US", { month: "short", day: "numeric" });
      const cond = WMO_CODES[d.weather_code[i]] || "Unknown";
      const rainChance = d.precipitation_probability_max[i];
      forecast += `${dayName} ${dateLabel}: High ${Math.round(d.temperature_2m_max[i])}Â°F / Low ${Math.round(d.temperature_2m_min[i])}Â°F â€” ${cond}${rainChance > 20 ? ` (${rainChance}% rain)` : ""}\n`;
    }

    return forecast;
  } catch {
    return null;
  }
}

// â”€â”€ Follow-Up Suggestions Parser â”€â”€
// Strips the ---FOLLOWUPS--- block from LLM responses and returns them as a separate array.
// Also detects trailing numbered questions even without the marker as a fallback.
function parseFollowUps(raw: string): { clean: string; followUps: string[] } {
  // Primary: look for explicit ---FOLLOWUPS--- marker
  const idx = raw.indexOf("---FOLLOWUPS---");
  if (idx !== -1) {
    const clean = raw.slice(0, idx).trimEnd();
    const followUps = raw.slice(idx + 15).trim()
      .split("\n")
      .map(l => l.replace(/^\d+\.\s*/, "").trim())
      .filter(l => l.length > 0 && l.length <= 80)
      .slice(0, 3);
    return { clean, followUps };
  }

  // Fallback: detect trailing numbered questions (1. ...? 2. ...? 3. ...?)
  // This catches cases where the LLM skips the marker but still appends follow-ups.
  const lines = raw.trimEnd().split("\n");
  const trailingQ: string[] = [];
  // Walk backwards from end, collecting numbered question lines
  for (let i = lines.length - 1; i >= 0; i--) {
    const line = lines[i].trim();
    if (!line) continue; // skip blank lines at the end
    const m = line.match(/^\d+\.\s+(.+\?)\s*$/);
    if (m && m[1].length <= 80) {
      trailingQ.unshift(m[1]);
    } else {
      break; // stop at first non-question line
    }
  }
  if (trailingQ.length >= 2 && trailingQ.length <= 4) {
    // Strip the trailing question lines from the response
    let cutFrom = lines.length;
    for (let i = lines.length - 1; i >= 0; i--) {
      const line = lines[i].trim();
      if (!line) continue;
      if (/^\d+\.\s+.+\?\s*$/.test(line)) {
        cutFrom = i;
      } else {
        break;
      }
    }
    // Also strip any blank lines right before the questions
    while (cutFrom > 0 && !lines[cutFrom - 1].trim()) cutFrom--;
    const clean = lines.slice(0, cutFrom).join("\n").trimEnd();
    return { clean, followUps: trailingQ.slice(0, 3) };
  }

  return { clean: raw, followUps: [] };
}

/**
 * Post-process LLM responses to strip leaked system prompt artifacts.
 * Sometimes LLMs echo back internal instructions, markers, or fabricated links.
 */
function cleanLLMResponse(text: string): string {
  return text
    // Strip leaked HammerLock AI Response headers
    .replace(/---\s*HammerLock AI Response.*?---/gi, "")
    // Strip inline (FOLLOWUPS) markers the LLM might output
    .replace(/\(FOLLOWUPS?\)/gi, "")
    // Strip "Given your specifications..." system echo patterns
    .replace(/Given your specifications for responses involving HammerLock AI['']s operations.*?(?:\n|$)/gi, "")
    // Strip leaked system context references
    .replace(/while ensuring user privacy and providing localized information relevant to.*?(?::\n|:\s|$)/gi, "")
    // Strip placeholder/fabricated links like [Watch Here], [Link], [Link to ...]
    .replace(/\[(?:Watch Here|Link(?:\s+to\s+[^\]]*)?|Click Here|Link to YouTube Video)\]/gi, "")
    // Strip remaining ---...--- section dividers that aren't markdown horizontal rules
    .replace(/---[A-Z][A-Za-z\s]*---/g, "")
    // Clean up resulting double-spaces and empty lines
    .replace(/\n{3,}/g, "\n\n")
    .replace(/[^\S\n]{2,}/g, " ")
    .trim();
}

/** Strip HTML tags from a string (for Brave API snippets) */
function stripHtml(s: string): string {
  return s.replace(/<[^>]*>/g, "");
}

// â”€â”€ Persona Context Builder â”€â”€
// Dynamically parses ANY user's persona and injects relevant context
// for planning/suggestion/recommendation queries. Works for any location,
// any family situation â€” no hardcoded data.

const PLANNING_KEYWORDS = /\b(family|today|tonight|this weekend|do|plan|activity|activities|fun|play|go out|outing|trip|adventure|things to do|date night|kids?|bored|suggestions?|recommend|where should|what should|places?|restaurant|eat|dinner|lunch|park|outdoor|indoor)\b/i;

function parsePersonaDetails(persona: string): {
  location: string | null;
  constraints: string[];
  preferences: string[];
} {
  const location = extractUserLocation(persona);
  const constraints: string[] = [];
  const preferences: string[] = [];

  // Family structure
  const kidsMatch = persona.match(/(\d+)\s*kids?/i);
  if (kidsMatch) {
    constraints.push(`${kidsMatch[1]} kids â€” suggest kid-friendly, age-appropriate options`);
  } else if (/\bkid|child|son|daughter/i.test(persona)) {
    constraints.push("has kids â€” suggest kid-friendly options");
  }

  // Pregnancy / mobility
  if (/\bpregnant/i.test(persona)) {
    constraints.push("partner is pregnant â€” prioritize low-impact, comfortable, seated or short-walk options; avoid extreme heat, crowds, heavy physical activity");
  }

  // Baby / toddler
  if (/\bbaby|infant|toddler|newborn/i.test(persona)) {
    constraints.push("has a baby/toddler â€” needs stroller-friendly, quieter venues with changing facilities");
  }

  // Pets
  if (/\bdog|cat|pet/i.test(persona)) {
    constraints.push("has pets â€” may prefer pet-friendly venues when relevant");
  }

  // Dietary preferences
  const dietMatch = persona.match(/\b(vegan|vegetarian|gluten[- ]?free|kosher|halal|dairy[- ]?free|allergic to \w+)/i);
  if (dietMatch) {
    preferences.push(`dietary: ${dietMatch[1]}`);
  }

  // Communication style
  const styleMatch = persona.match(/communication style[:\s]+(\w+)/i);
  if (styleMatch) {
    preferences.push(`communication style: ${styleMatch[1]}`);
  }

  return { location, constraints, preferences };
}

function buildFamilyContext(prompt: string, persona: string): string {
  // Only inject for planning/suggestion/recommendation queries
  if (!PLANNING_KEYWORDS.test(prompt)) return "";
  if (!persona.trim()) return "";

  const { location, constraints, preferences } = parsePersonaDetails(persona);

  // Don't inject empty context
  if (!location && constraints.length === 0 && preferences.length === 0) return "";

  let block = "\n\n--- USER CONTEXT (personalize your answer using this) ---\n";
  if (location) {
    block += `ðŸ“ Location: ${location} â€” name SPECIFIC real places, venues, parks, and businesses in or near this area. Be local, not generic.\n`;
  }
  if (constraints.length > 0) {
    block += `âš ï¸ Constraints:\n`;
    constraints.forEach(c => { block += `  - ${c}\n`; });
  }
  if (preferences.length > 0) {
    block += `ðŸ’¡ Preferences: ${preferences.join("; ")}\n`;
  }
  if (location) {
    block += `\nðŸ” If you're not confident about specific venue names in ${location}, suggest the user ask you to "search for [topic] near me" to get real-time results.\n`;
  }
  block += "--- END USER CONTEXT ---";
  return block;
}

async function callOllama(systemPrompt: string, prompt: string) {
  // Skip Ollama in serverless environments â€” localhost is not available
  const isServerless = !!process.env.VERCEL || !!process.env.AWS_LAMBDA_FUNCTION_NAME;
  if (isServerless) return null;

  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 30000);
    const response = await fetch(`${OLLAMA_BASE_URL}/api/chat`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        model: OLLAMA_MODEL,
        messages: [
          { role: "system", content: systemPrompt },
          { role: "user", content: prompt }
        ],
        stream: false
      }),
      signal: controller.signal
    });
    clearTimeout(timeoutId);
    if (!response.ok) return null;
    const data = await response.json();
    return data.message?.content?.trim() || null;
  } catch (error) {
    console.error("Ollama call failed, falling back:", (error as Error).message);
    return null;
  }
}

async function callOpenAI(systemPrompt: string, prompt: string) {
  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey) return null;
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 25000);
    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "Authorization": `Bearer ${apiKey}`,
      },
      body: JSON.stringify({
        model: process.env.OPENAI_MODEL || "gpt-4o-mini",
        messages: [
          { role: "system", content: systemPrompt },
          { role: "user", content: prompt },
        ],
      }),
      signal: controller.signal,
    });
    clearTimeout(timeoutId);
    if (!response.ok) {
      const errBody = await response.text();
      lastLLMError = `OpenAI ${response.status}: ${errBody.slice(0, 200)}`;
      console.error("[execute] OpenAI API error:", response.status, errBody.slice(0, 300));
      return null;
    }
    const data = await response.json();
    const content = data.choices?.[0]?.message?.content?.trim();
    if (!content) {
      lastLLMError = "OpenAI returned empty response";
      return null;
    }
    return content;
  } catch (error) {
    lastLLMError = `OpenAI: ${(error as Error).message}`;
    console.error("OpenAI call failed, falling back:", (error as Error).message);
    return null;
  }
}

async function callAnthropic(systemPrompt: string, prompt: string) {
  const apiKey = process.env.ANTHROPIC_API_KEY;
  if (!apiKey) return null;
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 25000);
    const response = await fetch("https://api.anthropic.com/v1/messages", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "x-api-key": apiKey,
        "anthropic-version": "2023-06-01",
      },
      body: JSON.stringify({
        model: process.env.ANTHROPIC_MODEL || "claude-sonnet-4-5-20250929",
        max_tokens: 2000,
        system: systemPrompt,
        messages: [{ role: "user", content: prompt }],
      }),
      signal: controller.signal,
    });
    clearTimeout(timeoutId);
    if (!response.ok) {
      const errBody = await response.text();
      lastLLMError = `Anthropic ${response.status}: ${errBody.slice(0, 200)}`;
      console.error("[execute] Anthropic API error:", response.status, errBody.slice(0, 300));
      return null;
    }
    const data = await response.json();
    return (
      data.content
        ?.map((part: { type: string; text?: string }) => (part.type === "text" ? part.text : ""))
        .join("\n")
        .trim() || null
    );
  } catch (error) {
    lastLLMError = `Anthropic: ${(error as Error).message}`;
    console.error("Anthropic call failed, falling back:", (error as Error).message);
    return null;
  }
}

async function callGemini(systemPrompt: string, prompt: string) {
  const apiKey = process.env.GEMINI_API_KEY;
  if (!apiKey) return null;
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 25000);
    const model = process.env.GEMINI_MODEL || "gemini-2.0-flash";
    const response = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`,
      {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          systemInstruction: { parts: [{ text: systemPrompt }] },
          contents: [{ role: "user", parts: [{ text: prompt }] }],
        }),
        signal: controller.signal,
      }
    );
    clearTimeout(timeoutId);
    if (!response.ok) {
      const errBody = await response.text();
      lastLLMError = `Gemini ${response.status}: ${errBody.slice(0, 200)}`;
      console.error("[execute] Gemini API error:", response.status, errBody.slice(0, 300));
      return null;
    }
    const data = await response.json();
    const text = data.candidates?.[0]?.content?.parts?.[0]?.text?.trim();
    if (!text) {
      lastLLMError = "Gemini returned empty response";
      return null;
    }
    return text;
  } catch (error) {
    lastLLMError = `Gemini: ${(error as Error).message}`;
    console.error("Gemini call failed, falling back:", (error as Error).message);
    return null;
  }
}

async function callGroq(systemPrompt: string, prompt: string) {
  const apiKey = process.env.GROQ_API_KEY;
  if (!apiKey) return null;
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 25000);
    // Groq uses OpenAI-compatible API
    const response = await fetch("https://api.groq.com/openai/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "Authorization": `Bearer ${apiKey}`,
      },
      body: JSON.stringify({
        model: process.env.GROQ_MODEL || "llama-3.3-70b-versatile",
        messages: [
          { role: "system", content: systemPrompt },
          { role: "user", content: prompt },
        ],
      }),
      signal: controller.signal,
    });
    clearTimeout(timeoutId);
    if (!response.ok) {
      const errBody = await response.text();
      lastLLMError = `Groq ${response.status}: ${errBody.slice(0, 200)}`;
      console.error("[execute] Groq API error:", response.status, errBody.slice(0, 300));
      return null;
    }
    const data = await response.json();
    const content = data.choices?.[0]?.message?.content?.trim();
    if (!content) {
      lastLLMError = "Groq returned empty response";
      return null;
    }
    return content;
  } catch (error) {
    lastLLMError = `Groq: ${(error as Error).message}`;
    console.error("Groq call failed, falling back:", (error as Error).message);
    return null;
  }
}

async function callMistral(systemPrompt: string, prompt: string) {
  const apiKey = process.env.MISTRAL_API_KEY;
  if (!apiKey) return null;
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 25000);
    const response = await fetch("https://api.mistral.ai/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "Authorization": `Bearer ${apiKey}`,
      },
      body: JSON.stringify({
        model: process.env.MISTRAL_MODEL || "mistral-small-latest",
        messages: [
          { role: "system", content: systemPrompt },
          { role: "user", content: prompt },
        ],
      }),
      signal: controller.signal,
    });
    clearTimeout(timeoutId);
    if (!response.ok) {
      const errBody = await response.text();
      lastLLMError = `Mistral ${response.status}: ${errBody.slice(0, 200)}`;
      console.error("[execute] Mistral API error:", response.status, errBody.slice(0, 300));
      return null;
    }
    const data = await response.json();
    const content = data.choices?.[0]?.message?.content?.trim();
    if (!content) {
      lastLLMError = "Mistral returned empty response";
      return null;
    }
    return content;
  } catch (error) {
    lastLLMError = `Mistral: ${(error as Error).message}`;
    console.error("Mistral call failed, falling back:", (error as Error).message);
    return null;
  }
}

async function callDeepSeek(systemPrompt: string, prompt: string) {
  const apiKey = process.env.DEEPSEEK_API_KEY;
  if (!apiKey) return null;
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 25000);
    // DeepSeek uses OpenAI-compatible API
    const response = await fetch("https://api.deepseek.com/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "Authorization": `Bearer ${apiKey}`,
      },
      body: JSON.stringify({
        model: process.env.DEEPSEEK_MODEL || "deepseek-chat",
        messages: [
          { role: "system", content: systemPrompt },
          { role: "user", content: prompt },
        ],
      }),
      signal: controller.signal,
    });
    clearTimeout(timeoutId);
    if (!response.ok) {
      const errBody = await response.text();
      lastLLMError = `DeepSeek ${response.status}: ${errBody.slice(0, 200)}`;
      console.error("[execute] DeepSeek API error:", response.status, errBody.slice(0, 300));
      return null;
    }
    const data = await response.json();
    const content = data.choices?.[0]?.message?.content?.trim();
    if (!content) {
      lastLLMError = "DeepSeek returned empty response";
      return null;
    }
    return content;
  } catch (error) {
    lastLLMError = `DeepSeek: ${(error as Error).message}`;
    console.error("DeepSeek call failed, falling back:", (error as Error).message);
    return null;
  }
}

// Track the last LLM error for better diagnostics in serverless
let lastLLMError: string | null = null;

async function callGateway(prompt: string): Promise<string> {
  // Skip CLI gateway in serverless environments (it doesn't exist there)
  const isServerless = !!process.env.VERCEL || !!process.env.AWS_LAMBDA_FUNCTION_NAME;
  if (isServerless) {
    const detail = lastLLMError ? ` Last error: ${lastLLMError}` : "";
    throw new Error(
      `No LLM provider responded.${detail} Check that your API key is valid.`
    );
  }

  try {
    const escaped = prompt.replace(/'/g, "'\\''");
    const { stdout } = await execAsync(
      `openclaw --profile hammerlock agent --agent main --message '${escaped}' --json --no-color`,
      { timeout: 30000 }
    );
    const result = JSON.parse(stdout);
    if (result.status === "ok") {
      return result.result?.payloads?.[0]?.text || "No response from gateway agent";
    }
    throw new Error(result.summary || "Gateway agent failed");
  } catch (error) {
    throw new Error(
      `No LLM provider configured. Set OPENAI_API_KEY or ANTHROPIC_API_KEY, or ensure the OpenClaw gateway is running. (${(error as Error).message})`
    );
  }
}

// â”€â”€ OpenClaw Action Execution â”€â”€
// Enhanced gateway call for skill-based actions (reminders, email, smart home, etc.)

type GatewayActionResult = {
  response: string;
  actionType: string;
  success: boolean;
};

async function callGatewayAction(
  message: string,
  actionType: string
): Promise<GatewayActionResult> {
  const isServerless = !!process.env.VERCEL || !!process.env.AWS_LAMBDA_FUNCTION_NAME;
  if (isServerless) {
    return {
      response: "Action execution requires the desktop app. OpenClaw isn't available in the web version.",
      actionType,
      success: false,
    };
  }

  // Fast health check â€” avoid 60s hang on dead gateway
  try {
    await execAsync("openclaw --profile hammerlock health --json 2>/dev/null", { timeout: 4000 });
  } catch {
    return {
      response: "The OpenClaw gateway is offline. Start it with `openclaw gateway` and try again.",
      actionType,
      success: false,
    };
  }

  try {
    const escaped = message.replace(/'/g, "'\\''");
    const { stdout } = await execAsync(
      `openclaw --profile hammerlock agent --agent main --message '${escaped}' --json --no-color`,
      { timeout: 60000 }
    );
    const result = JSON.parse(stdout);
    if (result.status === "ok") {
      const text = result.result?.payloads?.[0]?.text || "Action completed.";
      return { response: text, actionType, success: true };
    }
    return {
      response: result.summary || `The ${actionType} action didn't complete. Try again?`,
      actionType,
      success: false,
    };
  } catch (error) {
    const msg = (error as Error).message;
    if (msg.includes("timeout")) {
      return {
        response: `The ${actionType} action timed out. Try again or check if OpenClaw is running.`,
        actionType,
        success: false,
      };
    }
    return {
      response: `Couldn't complete the ${actionType} action: ${msg}`,
      actionType,
      success: false,
    };
  }
}

type ChatMessage = { role: string; content: string };

// HammerLock AI core identity â€” this is what the LLM knows about itself
const HAMMERLOCK_IDENTITY = `You are HammerLock AI â€” a personal AI assistant that lives on the user's desktop. You can do everything: answer questions, have conversations, search the web, analyze documents, remember user preferences, help with any task.

## YOUR TOOLS (these work right now, not hypothetically)
1. **Web search** â€” Brave Search is wired in. When the user asks about weather, news, prices, events, or anything real-time, search results get injected into your prompt. USE THEM. Present the data directly. Never say "I can't access the web" â€” you literally just searched it.
2. **Time/date** â€” A CURRENT TIME block gets injected when the user asks for the time. Read it literally and answer with the exact time first.
3. **Memory** â€” The user can say "remember: ..." to save preferences, facts, and context. This persists across conversations. Teach users about this when they ask how to personalize you.
4. **PDF analysis** â€” Users can upload PDFs and ask questions about them.
5. **Image/Vision** â€” Users can upload images (screenshots, photos, etc.) and you CAN see and describe them. When an image is attached, describe what you see in detail. Never say "I can't view images" â€” the image is sent directly to you.
6. **Reports** â€” You can generate summaries and reports from conversations.

## HOW TO BE
- Talk like a smart, helpful friend â€” warm but not cheesy
- Short answers for simple questions. Detailed for complex ones. Match the energy.
- Use the user's name, location, family details, and preferences naturally â€” they're provided in your context
- Name real specific places, not generic suggestions. "Central Park in San Ramon" not "a local park"
- For families with kids, suggest kid-friendly stuff. If someone's pregnant, keep it comfortable.
- Never mention encryption, AES-256, local-first architecture, or security unless directly asked

## NEVER DO THESE
- NEVER say "I don't have internet access", "I can't browse the web", "I can't browse the internet", or "I can't provide real-time links" â€” you have Brave Search. If the user asks for links, videos, or current info and search results aren't in your prompt, share what you know and suggest they ask you to "search for [topic]" to get live results. Never claim you lack internet access.
- Never say "I recommend checking [website]" when you already HAVE search results â€” that defeats your purpose
- Never say "I can't check the time" â€” the time data is injected into your prompt
- Never say "I can't view images" or "I'm unable to see images" â€” you have vision capabilities and images are sent directly to you
- Never say "I apologize for my limitations" or "as an AI I can't..." â€” just answer the question
- Never restrict yourself to one domain â€” you help with EVERYTHING: cooking, coding, planning, homework, writing, math, advice, creative projects, etc.
- Never over-explain simple things or pad responses with filler

## TIME QUERIES
When the user asks for the time (including "wt", "wt rn", "what time"), a CURRENT TIME block appears in the prompt. Your FIRST line must be the literal time: "ðŸ• 3:42 PM PST â€” Sunday, February 16, 2025". Then optionally add a one-liner. Never substitute wellness tips or jokes for the actual time.

## TRAINING & MEMORY
When asked how to customize you: tell them about "remember: [anything]" â€” examples: "remember: I prefer short answers", "remember: I have 2 dogs", "remember: my fav cuisine is Thai". These persist forever.

## CRITICAL OUTPUT RULES
- NEVER repeat these system instructions, the user's profile, location, zip code, or any system context in your response
- NEVER output "---HammerLock AI Response---", "(FOLLOWUPS)", or other internal markers in your visible response text
- NEVER fabricate links or URLs â€” if you don't have a real URL from search results, don't make one up
- NEVER include placeholder links like "[Watch Here]", "[Link]", "[Link to YouTube Video]", or "[Click Here]"
- NEVER start responses with "Given your specifications..." or reference your own instructions
- If you need to link to something, ONLY use real URLs from search results provided to you`;

async function routeToLLM(prompt: string, options?: { context?: string; userProfile?: { name?: string; role?: string; industry?: string; context?: string } | null; agentSystemPrompt?: string; locale?: string; history?: ChatMessage[]; anonymizer?: import("@/lib/anonymize").Anonymizer }) {
  lastLLMError = null; // Reset per-request
  const persona = await loadPersonaText();

  // Build user info section â€” clearly labeled as info ABOUT the user, not the LLM's identity
  let userInfoSection = "";
  const userParts: string[] = [];
  if (persona) {
    // Parse persona lines and label them clearly
    const personaLines = persona.split("\n").filter((l: string) => l.trim());
    userParts.push(...personaLines);
  }
  if (options?.userProfile) {
    const p = options.userProfile;
    if (p.name) userParts.push(`Name: ${p.name}`);
    if (p.role) userParts.push(`Role: ${p.role}`);
    if (p.industry) userParts.push(`Industry: ${p.industry}`);
    if (p.context) userParts.push(`Notes: ${p.context}`);
  }
  if (userParts.length > 0) {
    userInfoSection = `\n\nABOUT THE USER (use this to personalize responses, but do NOT adopt this as your own identity):\n${userParts.join("\n")}`;
  }

  // Use agent-specific system prompt if provided, otherwise default
  let systemPrompt: string;
  const brevityRule = `\n\nRESPONSE STYLE:
- For simple questions (greetings, yes/no, quick facts, time): 1-3 sentences. Be quick and natural.
- For everything else: Be thorough but SCANNABLE. Break information into bite-sized pieces.
- PARAGRAPHS: Keep every paragraph to 2-3 sentences MAX. Add a blank line between each paragraph. Walls of text are never acceptable.
- Use markdown headings with emoji numbers for sections: "## 1ï¸âƒ£ Section Title" (always use ## heading syntax, never just bold text)
- Always put a blank line before and after headings, lists, and code blocks
- Prefer bullet points and short paragraphs over dense prose â€” readers skim, not read
- Use markdown tables when comparing options, features, or tradeoffs
- Use âœ… / âŒ for pros/cons lists
- Use **bold** for key terms, facts, and numbers
- Use bullet points (-) consistently for unordered lists, numbered lists (1. 2. 3.) for sequential steps
- End substantive answers with a brief personal take â€” not just dry facts
- Tone: warm, direct, professional. Like a brilliant friend who happens to be an expert.
- When in doubt, break it up more. Short chunks > long blocks.`;

  // Language rule: always respond in the user's selected UI language
  const uiLang = options?.locale ? (LOCALE_LANG[options.locale] || "English") : "English";
  const langRule = `\n\nLANGUAGE: Respond in ${uiLang}.`;

  const followUpRule = `\n\nFOLLOW-UP SUGGESTIONS:
At the very end of every response, add this block:
---FOLLOWUPS---
1. [First follow-up question]
2. [Second follow-up question]
3. [Third follow-up question]

Rules:
- 2-3 short questions the user might ask next (under 60 chars each)
- Contextually relevant to what you just answered
- Written as the USER would ask them (first person)
- Skip this block entirely for simple utility answers (time, status, greetings, confirmations like "Got it!" or "Saved!")`;

  if (options?.agentSystemPrompt) {
    // For search-specific system prompts (contain SEARCH RESULTS), skip followUpRule
    // since search follow-ups are generated server-side for reliability
    const isSearchPrompt = options.agentSystemPrompt.includes("SEARCH RESULTS");
    systemPrompt = options.agentSystemPrompt + userInfoSection + brevityRule + langRule + (isSearchPrompt ? "" : followUpRule);
  } else {
    systemPrompt = HAMMERLOCK_IDENTITY + userInfoSection + brevityRule + langRule + followUpRule;
  }

  // Build conversation history for multi-turn context
  const history = options?.history || [];
  let userPrompt = options?.context ? `${options.context}\n\n${prompt}` : prompt;

  // ---- Time context injection ----
  // If the prompt contains ANY time-related keywords, inject the real server clock
  // so the LLM actually knows the current time/date and can answer accurately.
  if (TIME_KEYWORDS.test(prompt)) {
    userPrompt = userPrompt + buildTimeContext(persona);
  }

  // ---- Family & location context injection ----
  // For planning/suggestion queries, prepend localized family context + local venue data
  const familyContext = buildFamilyContext(prompt, persona);
  if (familyContext) {
    userPrompt = userPrompt + familyContext;
  }

  // ---- Anonymization layer (v2: outbound-only, known-PII-only) ----
  // Use the caller's anonymizer if provided (single instance pattern),
  // otherwise create one for non-search paths.
  const anon = options?.anonymizer ?? createAnonymizer(persona, options?.userProfile);

  // Cloud providers first (better quality, proper multi-turn support)
  // Only fall back to local Ollama if no cloud provider is configured
  const hasCloudProvider = !!(
    process.env.OPENAI_API_KEY ||
    process.env.ANTHROPIC_API_KEY ||
    process.env.GEMINI_API_KEY ||
    process.env.GROQ_API_KEY ||
    process.env.MISTRAL_API_KEY ||
    process.env.DEEPSEEK_API_KEY
  );

  // ONLY scrub the outbound USER message â€” never the system prompt
  // (system prompt contains search results, weather data, etc. that must stay intact)
  // and never the history (it's already been shown to the user).
  const scrubbedUser = anon.scrub(userPrompt);

  if (anon.detectedCount > 0) {
    console.log(`[anonymize] Scrubbed ${anon.detectedCount} PII items from outbound query (${anon.summary})`);
  }

  // Build messages array with history for cloud providers
  // Clean up any leftover placeholders from older conversations (legacy data)
  const historyMessages = history.map(m => {
    let content = m.content;
    if (typeof content === "string") {
      content = content
        .replace(/\[PERSON_\d+\]/g, "(name)")
        .replace(/\[ORG_\d+\]/g, "(business)")
        .replace(/\[(?:EMAIL|PHONE|SSN|CREDIT_CARD|ADDRESS|IP|DATE_OF_BIRTH|ACCOUNT)_\d+\]/g, "");
    }
    return { role: m.role === "user" ? "user" as const : "assistant" as const, content };
  });

  // Try cloud providers first (better quality + proper multi-turn context)
  if (hasCloudProvider) {
    const cloudReply =
      (await callOpenAIMulti(systemPrompt, historyMessages, scrubbedUser)) ??
      (await callAnthropicMulti(systemPrompt, historyMessages, scrubbedUser)) ??
      (await callGeminiMulti(systemPrompt, historyMessages, scrubbedUser)) ??
      (await callGroqMulti(systemPrompt, historyMessages, scrubbedUser)) ??
      (await callMistralMulti(systemPrompt, historyMessages, scrubbedUser)) ??
      (await callDeepSeekMulti(systemPrompt, historyMessages, scrubbedUser));

    // Restore any placeholders the LLM might echo back
    if (cloudReply) return anon.restore(cloudReply);
  }

  // Fallback to local Ollama (if no cloud provider available or all failed)
  const localPromptWithHistory = history.length > 0
    ? history.map(m => `${m.role === "user" ? "User" : "Assistant"}: ${m.content}`).join("\n") + `\nUser: ${userPrompt}`
    : userPrompt;
  const localReply = await callOllama(systemPrompt, localPromptWithHistory);
  if (localReply) return anon.restore(localReply);

  return await callGateway(userPrompt);
}

// Multi-turn wrappers for cloud providers
type MsgTurn = { role: "user" | "assistant"; content: string };

async function callOpenAIMulti(systemPrompt: string, history: MsgTurn[], userMsg: string) {
  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey) return null;
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 25000);

    // Detect embedded image data URLs for vision support
    const dataUrlMatch = userMsg.match(/(data:image\/[^;]+;base64,[A-Za-z0-9+/=]+)/);
    let userContent: string | Array<{ type: string; text?: string; image_url?: { url: string; detail?: string } }>;
    if (dataUrlMatch) {
      const imageUrl = dataUrlMatch[1];
      const textPart = userMsg.replace(imageUrl, "").replace(/\[Image attached:[^\]]*\]\s*/g, "").trim();
      userContent = [
        { type: "text", text: textPart || "Describe this image in detail." },
        { type: "image_url", image_url: { url: imageUrl, detail: "auto" } },
      ];
    } else {
      userContent = userMsg;
    }

    const messages = [
      { role: "system", content: systemPrompt },
      ...history,
      { role: "user", content: userContent },
    ];
    const model = process.env.OPENAI_MODEL || "gpt-4o-mini";
    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: { "Content-Type": "application/json", "Authorization": `Bearer ${apiKey}` },
      body: JSON.stringify({ model, messages, ...(dataUrlMatch ? { max_tokens: 1000 } : {}) }),
      signal: controller.signal,
    });
    clearTimeout(timeoutId);
    if (!response.ok) {
      const errBody = await response.text();
      lastLLMError = `OpenAI ${response.status}: ${errBody.slice(0, 200)}`;
      return null;
    }
    const data = await response.json();
    return data.choices?.[0]?.message?.content?.trim() || null;
  } catch (error) {
    lastLLMError = `OpenAI: ${(error as Error).message}`;
    return null;
  }
}

async function callAnthropicMulti(systemPrompt: string, history: MsgTurn[], userMsg: string) {
  const apiKey = process.env.ANTHROPIC_API_KEY;
  if (!apiKey) return null;
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 25000);
    const messages = [...history, { role: "user" as const, content: userMsg }];
    const response = await fetch("https://api.anthropic.com/v1/messages", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        "x-api-key": apiKey,
        "anthropic-version": "2023-06-01",
      },
      body: JSON.stringify({
        model: process.env.ANTHROPIC_MODEL || "claude-sonnet-4-5-20250929",
        max_tokens: 2000,
        system: systemPrompt,
        messages,
      }),
      signal: controller.signal,
    });
    clearTimeout(timeoutId);
    if (!response.ok) {
      const errBody = await response.text();
      lastLLMError = `Anthropic ${response.status}: ${errBody.slice(0, 200)}`;
      return null;
    }
    const data = await response.json();
    return data.content?.map((part: { type: string; text?: string }) => (part.type === "text" ? part.text : "")).join("\n").trim() || null;
  } catch (error) {
    lastLLMError = `Anthropic: ${(error as Error).message}`;
    return null;
  }
}

async function callGeminiMulti(systemPrompt: string, history: MsgTurn[], userMsg: string) {
  const apiKey = process.env.GEMINI_API_KEY;
  if (!apiKey) return null;
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 25000);
    const model = process.env.GEMINI_MODEL || "gemini-2.0-flash";
    const contents = [
      ...history.map(m => ({ role: m.role === "user" ? "user" : "model", parts: [{ text: m.content }] })),
      { role: "user", parts: [{ text: userMsg }] },
    ];
    const response = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`,
      {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          systemInstruction: { parts: [{ text: systemPrompt }] },
          contents,
        }),
        signal: controller.signal,
      }
    );
    clearTimeout(timeoutId);
    if (!response.ok) {
      const errBody = await response.text();
      lastLLMError = `Gemini ${response.status}: ${errBody.slice(0, 200)}`;
      return null;
    }
    const data = await response.json();
    return data.candidates?.[0]?.content?.parts?.[0]?.text?.trim() || null;
  } catch (error) {
    lastLLMError = `Gemini: ${(error as Error).message}`;
    return null;
  }
}

async function callGroqMulti(systemPrompt: string, history: MsgTurn[], userMsg: string) {
  const apiKey = process.env.GROQ_API_KEY;
  if (!apiKey) return null;
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 25000);
    const messages = [
      { role: "system", content: systemPrompt },
      ...history,
      { role: "user", content: userMsg },
    ];
    const response = await fetch("https://api.groq.com/openai/v1/chat/completions", {
      method: "POST",
      headers: { "Content-Type": "application/json", "Authorization": `Bearer ${apiKey}` },
      body: JSON.stringify({ model: process.env.GROQ_MODEL || "llama-3.3-70b-versatile", messages }),
      signal: controller.signal,
    });
    clearTimeout(timeoutId);
    if (!response.ok) {
      const errBody = await response.text();
      lastLLMError = `Groq ${response.status}: ${errBody.slice(0, 200)}`;
      return null;
    }
    const data = await response.json();
    return data.choices?.[0]?.message?.content?.trim() || null;
  } catch (error) {
    lastLLMError = `Groq: ${(error as Error).message}`;
    return null;
  }
}

async function callMistralMulti(systemPrompt: string, history: MsgTurn[], userMsg: string) {
  const apiKey = process.env.MISTRAL_API_KEY;
  if (!apiKey) return null;
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 25000);
    const messages = [
      { role: "system", content: systemPrompt },
      ...history,
      { role: "user", content: userMsg },
    ];
    const response = await fetch("https://api.mistral.ai/v1/chat/completions", {
      method: "POST",
      headers: { "Content-Type": "application/json", "Authorization": `Bearer ${apiKey}` },
      body: JSON.stringify({ model: process.env.MISTRAL_MODEL || "mistral-small-latest", messages }),
      signal: controller.signal,
    });
    clearTimeout(timeoutId);
    if (!response.ok) {
      const errBody = await response.text();
      lastLLMError = `Mistral ${response.status}: ${errBody.slice(0, 200)}`;
      return null;
    }
    const data = await response.json();
    return data.choices?.[0]?.message?.content?.trim() || null;
  } catch (error) {
    lastLLMError = `Mistral: ${(error as Error).message}`;
    return null;
  }
}

async function callDeepSeekMulti(systemPrompt: string, history: MsgTurn[], userMsg: string) {
  const apiKey = process.env.DEEPSEEK_API_KEY;
  if (!apiKey) return null;
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 25000);
    const messages = [
      { role: "system", content: systemPrompt },
      ...history,
      { role: "user", content: userMsg },
    ];
    const response = await fetch("https://api.deepseek.com/chat/completions", {
      method: "POST",
      headers: { "Content-Type": "application/json", "Authorization": `Bearer ${apiKey}` },
      body: JSON.stringify({ model: process.env.DEEPSEEK_MODEL || "deepseek-chat", messages }),
      signal: controller.signal,
    });
    clearTimeout(timeoutId);
    if (!response.ok) {
      const errBody = await response.text();
      lastLLMError = `DeepSeek ${response.status}: ${errBody.slice(0, 200)}`;
      return null;
    }
    const data = await response.json();
    return data.choices?.[0]?.message?.content?.trim() || null;
  } catch (error) {
    lastLLMError = `DeepSeek: ${(error as Error).message}`;
    return null;
  }
}

const SEARCH_PATTERNS = [
  /^(?:web\s+)?search\s+(.+)/i,
  /^find\s+(.+)/i,
  /^latest on\s+(.+)/i,
  /^look up\s+(.+)/i
];

function extractSearchQuery(command: string): string | null {
  for (const pattern of SEARCH_PATTERNS) {
    const match = command.match(pattern);
    if (match && match[1]) {
      return match[1].trim();
    }
  }
  if (command.toLowerCase().includes(" search ")) {
    return command.replace(/.*search\s+/i, "").trim();
  }
  return null;
}

type BraveResult = {
  title: string;
  url: string;
  snippet: string;
  age: string;
  thumbnail: string;
  favicon: string;
  domain: string;
};

async function fetchBraveResults(query: string): Promise<BraveResult[]> {
  const braveKey = getBraveKey();
  if (!braveKey) {
    throw new Error("Add BRAVE_API_KEY to .env.local");
  }
  const controller = new AbortController();
  const timeout = setTimeout(() => controller.abort(), 8000);
  try {
    const res = await fetch(
      `https://api.search.brave.com/res/v1/web/search?q=${encodeURIComponent(query)}&count=10&country=US&search_lang=en`,
      {
        headers: {
          Accept: "application/json",
          "X-Subscription-Token": braveKey
        },
        signal: controller.signal
      }
    );
    if (!res.ok) {
      throw new Error(`Brave API error: ${res.status}`);
    }
    const data = await res.json();
    const items = data?.web?.results || [];
    return items.slice(0, 10).map((item: any) => {
      // Extract domain from URL for display
      let domain = "";
      try { domain = new URL(item?.url || "").hostname.replace(/^www\./, ""); } catch { /* ok */ }
      return {
        title: stripHtml(item?.title || item?.url || "Untitled result"),
        url: item?.url || "",
        snippet: stripHtml(item?.description || item?.snippet || "No snippet provided."),
        age: item?.page_age || item?.age || item?.publishedDate || "",
        thumbnail: item?.thumbnail?.src || item?.meta_url?.favicon || "",
        favicon: item?.meta_url?.favicon || "",
        domain,
      };
    });
  } catch (error) {
    if ((error as Error).name === "AbortError") {
      throw new Error("Search timed out");
    }
    throw error;
  } finally {
    clearTimeout(timeout);
  }
}

// Plain-text format sent to LLM for summarization
function formatBraveResults(results: BraveResult[], skipAge = false) {
  const lines = ["Results:"];
  results.forEach((result, idx) => {
    lines.push(`[${idx + 1}] ${result.title} - ${result.url}`);
    lines.push(`   ${result.snippet}`);
    if (result.age && !skipAge) lines.push(`   Published: ${result.age}`);
    lines.push("");
  });
  return lines.join("\n").trim();
}

// Generate follow-up suggestion chips server-side for search queries.
// More reliable than asking the LLM to produce them inline.
function generateSearchFollowUps(query: string): string[] {
  const q = query.toLowerCase();
  if (/weather|fore?cast|temperature/.test(q)) {
    return ["What's the weekly forecast?", "Will it rain tomorrow?", "What should I wear today?"];
  }
  if (/news|headlines/.test(q)) {
    return ["Tell me more about this", "Any other big stories today?", "Search for tech news"];
  }
  if (/restaurant|food|eat|dinner|lunch|bars?|cafe/i.test(q)) {
    return ["What else is nearby?", "What's open right now?", "Best rated ones?"];
  }
  if (/price|cost|how much/i.test(q)) {
    return ["Compare alternatives", "Where's the best deal?", "Any discounts available?"];
  }
  if (/recipe/i.test(q)) {
    return ["Any easier versions?", "What ingredients do I need?", "Show me a video"];
  }
  // Generic search follow-ups
  return ["Tell me more about this", "Search for something related", "What else should I know?"];
}

// Compact source list â€” small inline rows, no big images
function formatBraveResultsRich(results: BraveResult[]) {
  const rows = results.map((r, i) => {
    const age = r.age ? ` Â· ${r.age}` : "";
    // Compact row: number, linked title, domain badge, age
    return `${i + 1}. **[${r.title}](${r.url})** â€” *${r.domain}${age}*`;
  });
  return rows.join("\n");
}

// Map locale codes to full language names for LLM instruction
const LOCALE_LANG: Record<string, string> = {
  en: "English", "pt-BR": "Brazilian Portuguese", es: "Spanish",
  fr: "French", de: "German", zh: "Chinese", ja: "Japanese",
  ko: "Korean", ar: "Arabic", hi: "Hindi", ru: "Russian",
};

// Server-side i18n for API responses shown in chat feed
const API_STRINGS: Record<string, Record<string, string>> = {
  en: {
    no_command: "No command received.",
    no_search: "No search results found.",
    no_persona: "No persona set up yet. Tell me about yourself!",
    no_persona_alt: "No persona set up yet. Tell me about yourself and I'll remember it.",
    remember_saved: "Got it, I'll remember that:",
    remember_failed: "Couldn't save that right now. Try again?",
    credits_exhausted: "You've used all your included compute units. To keep going, you can **add your own API key** (sidebar > API Keys) for unlimited use, or **get more units** from the HammerLock AI store.",
    generic_error: "Something went wrong. Please try again.",
    your_persona: "Your Persona",
  },
  "pt-BR": {
    no_command: "Nenhum comando recebido.",
    no_search: "Nenhum resultado de busca encontrado.",
    no_persona: "Nenhum perfil configurado ainda. Me conte sobre vocÃª!",
    no_persona_alt: "Nenhum perfil configurado ainda. Me conte sobre vocÃª e eu vou lembrar.",
    remember_saved: "Entendi, vou lembrar disso:",
    remember_failed: "NÃ£o consegui salvar agora. Tenta de novo?",
    credits_exhausted: "VocÃª usou todas as suas unidades de computaÃ§Ã£o incluÃ­das. Para continuar, vocÃª pode **adicionar sua prÃ³pria chave API** (barra lateral > Chaves API) para uso ilimitado, ou **obter mais unidades** na loja HammerLock AI.",
    generic_error: "Algo deu errado. Tente novamente.",
    your_persona: "Seu Perfil",
  },
  es: {
    no_command: "No se recibiÃ³ ningÃºn comando.",
    no_search: "No se encontraron resultados de bÃºsqueda.",
    no_persona: "No hay persona configurada aÃºn. Â¡CuÃ©ntame sobre ti!",
    no_persona_alt: "No hay persona configurada aÃºn. CuÃ©ntame sobre ti y lo recordarÃ©.",
    remember_saved: "Entendido, recordarÃ© eso:",
    remember_failed: "No pude guardar eso ahora. Â¿Intentar de nuevo?",
    credits_exhausted: "Has usado todas tus unidades de cÃ³mputo incluidas. Para continuar, puedes **agregar tu propia clave API** (barra lateral > Claves API) para uso ilimitado.",
    generic_error: "Algo saliÃ³ mal. IntÃ©ntalo de nuevo.",
    your_persona: "Tu Persona",
  },
};

function apiStr(locale: string | undefined, key: string): string {
  const loc = locale && API_STRINGS[locale] ? locale : "en";
  return API_STRINGS[loc]?.[key] || API_STRINGS.en[key] || key;
}

// Well-known US city â†’ { state, zip } for search enrichment
const CITY_DATA: Record<string, { st: string; zip: string }> = {
  "san ramon": { st: "CA", zip: "94583" }, "san francisco": { st: "CA", zip: "94102" },
  "san jose": { st: "CA", zip: "95112" }, "los angeles": { st: "CA", zip: "90001" },
  "san diego": { st: "CA", zip: "92101" }, "sacramento": { st: "CA", zip: "95814" },
  "oakland": { st: "CA", zip: "94607" }, "palo alto": { st: "CA", zip: "94301" },
  "fremont": { st: "CA", zip: "94536" }, "walnut creek": { st: "CA", zip: "94596" },
  "dublin": { st: "CA", zip: "94568" }, "danville": { st: "CA", zip: "94526" },
  "pleasanton": { st: "CA", zip: "94566" }, "livermore": { st: "CA", zip: "94550" },
  "hayward": { st: "CA", zip: "94541" }, "berkeley": { st: "CA", zip: "94704" },
  "cupertino": { st: "CA", zip: "95014" }, "sunnyvale": { st: "CA", zip: "94086" },
  "mountain view": { st: "CA", zip: "94040" }, "santa clara": { st: "CA", zip: "95050" },
  "redwood city": { st: "CA", zip: "94061" }, "menlo park": { st: "CA", zip: "94025" },
  "concord": { st: "CA", zip: "94520" }, "antioch": { st: "CA", zip: "94509" },
  "new york": { st: "NY", zip: "10001" }, "brooklyn": { st: "NY", zip: "11201" },
  "manhattan": { st: "NY", zip: "10001" }, "queens": { st: "NY", zip: "11101" },
  "chicago": { st: "IL", zip: "60601" }, "houston": { st: "TX", zip: "77001" },
  "dallas": { st: "TX", zip: "75201" }, "austin": { st: "TX", zip: "78701" },
  "san antonio": { st: "TX", zip: "78201" }, "phoenix": { st: "AZ", zip: "85001" },
  "seattle": { st: "WA", zip: "98101" }, "portland": { st: "OR", zip: "97201" },
  "denver": { st: "CO", zip: "80201" }, "miami": { st: "FL", zip: "33101" },
  "tampa": { st: "FL", zip: "33601" }, "orlando": { st: "FL", zip: "32801" },
  "atlanta": { st: "GA", zip: "30301" }, "boston": { st: "MA", zip: "02101" },
  "detroit": { st: "MI", zip: "48201" }, "minneapolis": { st: "MN", zip: "55401" },
  "nashville": { st: "TN", zip: "37201" }, "las vegas": { st: "NV", zip: "89101" },
  "charlotte": { st: "NC", zip: "28201" }, "raleigh": { st: "NC", zip: "27601" },
  "columbus": { st: "OH", zip: "43201" }, "cleveland": { st: "OH", zip: "44101" },
  "pittsburgh": { st: "PA", zip: "15201" }, "philadelphia": { st: "PA", zip: "19101" },
  "baltimore": { st: "MD", zip: "21201" }, "washington": { st: "DC", zip: "20001" },
  "st louis": { st: "MO", zip: "63101" }, "kansas city": { st: "MO", zip: "64101" },
  "indianapolis": { st: "IN", zip: "46201" }, "milwaukee": { st: "WI", zip: "53201" },
  "salt lake city": { st: "UT", zip: "84101" },
};

// Compat wrapper
const CITY_STATE: Record<string, string> = Object.fromEntries(
  Object.entries(CITY_DATA).map(([k, v]) => [k, v.st])
);

// â”€â”€ Time utilities â”€â”€

/**
 * Get current time for a given IANA timezone.
 * Returns a structured object with formatted strings ready for display or prompt injection.
 */
function getCurrentTime(tz: string = "America/Los_Angeles") {
  const now = new Date();
  const timeStr = now.toLocaleTimeString("en-US", { timeZone: tz, hour: "numeric", minute: "2-digit", second: "2-digit", hour12: true });
  const dateStr = now.toLocaleDateString("en-US", { timeZone: tz, weekday: "long", month: "long", day: "numeric", year: "numeric" });
  const shortDate = now.toLocaleDateString("en-US", { timeZone: tz, month: "short", day: "numeric", year: "numeric" });
  const tzAbbr = now.toLocaleTimeString("en-US", { timeZone: tz, timeZoneName: "short" }).split(" ").pop() || tz;
  const hour24 = parseInt(now.toLocaleTimeString("en-US", { timeZone: tz, hour: "numeric", hour12: false }), 10);
  return { timeStr, dateStr, shortDate, tzAbbr, hour24, iso: now.toISOString() };
}

/** Convenience: get current time in user's inferred timezone (defaults PST). */
function getCurrentTimePST() {
  return getCurrentTime("America/Los_Angeles");
}

// â”€â”€ Time query detection & response â”€â”€

/** Keywords that signal ANY time-related intent (used for LLM prompt injection). */
const TIME_KEYWORDS = /\b(what\s*time|wh?at\s*tim|current\s*time|time\s*(is\s*it|now|rn|plz|pls)|wt\b|clock|what\s*(?:the\s+)?(?:date|day)|today(?:'?s)?\s*date|right\s*now)\b/i;

/**
 * Detect explicit "what time is it" queries that should be short-circuited
 * (answered instantly server-side without calling the LLM).
 * Much broader than before â€” catches slang, typos, abbreviations.
 */
function isTimeQuery(lowered: string): boolean {
  const cleaned = lowered.replace(/[?!.,;:'"]+/g, "").replace(/\s+/g, " ").trim();

  const exactPatterns = [
    // Core patterns
    /^wh?at\s*(?:time|tim)\s*(?:is\s*it|it\s*is|now|rn)?$/,
    /^(?:the\s+)?time\s*(?:now|rn|plz|pls)?$/,
    /^(?:tell\s+me\s+)?(?:the\s+)?(?:current\s+)?time\s*(?:rn)?$/,
    /^(?:current|exact)\s+time\s*(?:rn)?$/,
    // Abbreviations & slang
    /^w(?:hat)?t\s*(?:rn)?$/,                          // "wt", "wt rn", "wht rn"
    /^wt\s+(?:is\s+)?(?:the\s+)?(?:time|tim)\s*(?:rn)?$/, // "wt is the time rn"
    /^(?:whats?|wats?|wuts?)\s*(?:the\s+)?(?:time|tim)\s*(?:rn|now)?$/, // "whats the time rn"
    /^(?:yo\s+)?(?:whats?|wats?)\s+(?:the\s+)?time$/,  // "yo whats the time"
    /^tim\w*\s*(?:is|it)?\s*(?:now|rn)?$/,              // "time", "tim it is", "time rn"
    /^pst$/,                                             // just "pst"
    /^(?:got\s+)?(?:the\s+)?time\b/,                    // "got the time"
    /^(?:what\s+)?time\s+(?:is\s+it\s+)?(?:in|at)\s+\w+/, // "what time is it in SF"
    // Date queries
    /^(?:what|wats?|whats?)\s+(?:the\s+)?(?:date|day)\s*(?:today|now|rn|is\s*it)?$/,
    /^(?:today(?:'?s)?\s+)?date$/,
    /^what\s+day\s+is\s+(?:it|today)/,
    // "rn" (right now) â€” only when very short, likely asking for time
    /^(?:time|clock)\s+(?:check|rn)$/,
    // "what time" with garbage around it
    /^(?:hey\s+)?(?:what|wut|wat)\s*(?:'?s)?\s*(?:the\s+)?(?:time|tim|clock)\s*(?:rn|now|is\s*it)?$/,
  ];

  return exactPatterns.some(p => p.test(cleaned));
}

// Map common US location strings to IANA timezones
function guessTimezone(location: string | null): string {
  if (!location) return "America/Los_Angeles"; // Default PST
  const lower = location.toLowerCase();

  if (/\bca\b|california|san\s*(?:ramon|francisco|jose|diego)|los\s*angeles|sacramento|bay\s*area/.test(lower)) return "America/Los_Angeles";
  if (/\bny\b|new\s*york|brooklyn|manhattan|queens/.test(lower)) return "America/New_York";
  if (/\btx\b|texas|houston|dallas|austin|san\s*antonio/.test(lower)) return "America/Chicago";
  if (/\bfl\b|florida|miami|orlando|tampa/.test(lower)) return "America/New_York";
  if (/\bil\b|illinois|chicago/.test(lower)) return "America/Chicago";
  if (/\bwa\b|washington|seattle/.test(lower)) return "America/Los_Angeles";
  if (/\bco\b|colorado|denver/.test(lower)) return "America/Denver";
  if (/\baz\b|arizona|phoenix/.test(lower)) return "America/Phoenix";
  if (/\bhi\b|hawaii|honolulu/.test(lower)) return "Pacific/Honolulu";
  if (/\bak\b|alaska|anchorage/.test(lower)) return "America/Anchorage";
  if (/\b(?:pa|ma|md|dc|va|nc|sc|ga|ct|nj|de|ri|nh|vt|me)\b/.test(lower)) return "America/New_York";
  if (/\b(?:oh|mi|in|ky|tn|al|ms|wi|mn|ia|mo|ar|la|ne|ks|nd|sd|ok)\b/.test(lower)) return "America/Chicago";
  if (/\b(?:mt|id|wy|nm|ut)\b/.test(lower)) return "America/Denver";
  if (/\b(?:or|nv)\b/.test(lower)) return "America/Los_Angeles";

  return "America/Los_Angeles"; // Fallback
}

function buildTimeResponse(userLocation: string | null, _locale?: string): string {
  const tz = guessTimezone(userLocation);
  const { timeStr, dateStr, tzAbbr } = getCurrentTime(tz);
  const locationName = userLocation || "your area";
  return `ðŸ• **${timeStr} ${tzAbbr}** â€” ${dateStr}\n\nðŸ“ *${locationName}*`;
}

/**
 * Build a time context block to inject into the LLM prompt so the model
 * ACTUALLY KNOWS the current time/date when composing its response.
 * Called for any prompt that contains time-related keywords.
 */
function buildTimeContext(persona: string): string {
  const userLoc = extractUserLocation(persona);
  const tz = guessTimezone(userLoc);
  const { timeStr, dateStr, tzAbbr, hour24 } = getCurrentTime(tz);
  const locationName = userLoc || "unknown";

  let period = "daytime";
  if (hour24 < 6) period = "late night / early morning";
  else if (hour24 < 12) period = "morning";
  else if (hour24 < 17) period = "afternoon";
  else if (hour24 < 21) period = "evening";
  else period = "night";

  return `\n\n--- CURRENT TIME (factual, server-generated â€” use this to answer time questions) ---\nTime: ${timeStr} ${tzAbbr}\nDate: ${dateStr}\nTimezone: ${tz} (${tzAbbr})\nPeriod: ${period}\nUser location: ${locationName}\n--- END CURRENT TIME ---`;
}

// Extract user's saved location from persona text
function extractUserLocation(persona: string): string | null {
  const locMatch = persona.match(/(?:location|city|lives?\s+in|based\s+in|hometown)[:\s]+([^\n]+)/i);
  return locMatch ? locMatch[1].trim() : null;
}

// Enrich a search query with location context from persona / known cities
function enrichSearchQuery(query: string, persona: string): string {
  const lower = query.toLowerCase();
  const isWeather = /weather|fore?cast|temperature/i.test(query);
  const isLocal = /restaurants?|places?|things to do|events?|bars?|cafes?|shops?|near me/i.test(query);

  // Extract city from the query: "weather in San Ramon" â†’ "san ramon"
  // Also match "weather San Ramon CA" without preposition
  const cityMatch = lower.match(/(?:in|near|around|for)\s+([a-z\s]+?)(?:\s*(?:ca|tx|ny|fl|wa|il|az|or|co|ga|ma|mi|mn|tn|nv|nc|oh|pa|md|dc|mo|in|wi|ut)\b)?(?:\s*(?:today|tonight|now|this week|tomorrow))?[,?!.\s]*$/)
    || (isWeather && lower.match(/weather\s+([a-z\s]+?)(?:\s*(?:ca|tx|ny|fl|wa|il|az|or|co|ga|ma|mi|mn|tn|nv|nc|oh|pa|md|dc|mo|in|wi|ut)\b)?(?:\s*(?:today|tonight|now|this week|tomorrow))?[,?!.\s]*$/));

  if (cityMatch) {
    const cityName = cityMatch[1].trim().replace(/\s+(ca|tx|ny|fl|wa|il|az|or|co|ga|ma|mi|mn|tn|nv|nc|oh|pa|md|dc|mo|in|wi|ut)$/i, "").trim();
    const cityData = CITY_DATA[cityName];
    if (cityData) {
      const titleCity = cityName.split(" ").map(w => w[0].toUpperCase() + w.slice(1)).join(" ");
      const fullLocation = `${titleCity}, ${cityData.st} ${cityData.zip}`;
      if (isWeather) return `current weather ${fullLocation} today`;
      return query.replace(new RegExp(cityName.replace(/\s+/g, "\\s+"), "i"), fullLocation);
    }
  }

  // Replace "near me" with actual location for better local results
  if (/near me/i.test(query)) {
    const userLoc = extractUserLocation(persona);
    if (userLoc) {
      const cleanLoc = userLoc.replace(/\s*\d{5}(-\d{4})?\s*$/, "").trim();
      return query.replace(/near me/i, `near ${cleanLoc}`);
    }
  }

  // No city in query â€” try pulling location from persona
  if ((isWeather && !/\b(?:in|near|around|for)\s+[A-Za-z]/i.test(query) && !lower.match(/weather\s+[a-z]{3,}/))
    || (isLocal && !/\b(?:in|near|around)\s+[A-Za-z]/i.test(query))) {
    const userLoc = extractUserLocation(persona);
    if (userLoc) {
      if (isWeather) return `current weather in ${userLoc} today`;
      return `${query} in ${userLoc}`;
    }
  }

  // For weather queries, prepend "current" for better real-time results
  if (isWeather && !lower.includes("current")) {
    return `current ${query} today`;
  }

  return query;
}

// Boost direct weather/local results to the top
function sortSearchResults(results: BraveResult[], query: string): BraveResult[] {
  const lower = query.toLowerCase();
  const isWeather = /weather|fore?cast|temperature/i.test(lower);

  if (!isWeather) return results;

  // Preferred weather domains (in priority order)
  const weatherDomains = ["weather.com", "wunderground.com", "accuweather.com", "weather.gov", "weatherapi.com"];

  // Extract city and zip from query for URL matching
  const cityMatch = lower.match(/(?:in|near|around|for|weather)\s+([a-z\s]+?)(?:\s*,?\s*[a-z]{2}\s*)?(?:\s*(\d{5}))?\s*(?:today|tonight|now)?[,?!.\s]*$/);
  const citySlug = cityMatch ? cityMatch[1].trim().replace(/\s+/g, "-") : "";
  const zipCode = cityMatch?.[2] || "";

  return [...results].sort((a, b) => {
    const aDomain = weatherDomains.some(d => a.domain.includes(d));
    const bDomain = weatherDomains.some(d => b.domain.includes(d));
    const aUrl = a.url.toLowerCase();
    const bUrl = b.url.toLowerCase();
    const aLocal = (citySlug && aUrl.includes(citySlug)) || (zipCode && aUrl.includes(zipCode));
    const bLocal = (citySlug && bUrl.includes(citySlug)) || (zipCode && bUrl.includes(zipCode));

    // Best: weather domain + city/zip in URL
    if (aDomain && aLocal && !(bDomain && bLocal)) return -1;
    if (bDomain && bLocal && !(aDomain && aLocal)) return 1;
    // Next: weather domain
    if (aDomain && !bDomain) return -1;
    if (bDomain && !aDomain) return 1;
    // Next: city/zip in URL
    if (aLocal && !bLocal) return -1;
    if (bLocal && !aLocal) return 1;
    return 0;
  });
}

// Detect questions that need real-time web info (weather, events, news, prices, etc.)
function needsWebSearch(text: string): string | null {
  const lower = text.toLowerCase().trim();

  // Skip very short messages, greetings, and meta-commands
  if (lower.length < 10) return null;
  // Skip long-form text â€” likely a document paste, persona dump, or essay, not a search query
  if (lower.length > 500) return null;
  if (/^(hi|hello|hey|sup|yo|thanks|thank you|ok|okay|yes|no|bye|status|help)\b/i.test(lower)) return null;
  if (/^(summarize|generate|remember|load|read file|export)/i.test(lower)) return null;

  // STRONG triggers â€” these almost always need web search
  const strongPatterns = [
    /weather/i,
    /fore?cast/i,
    /\bnews\b/i,
    /headlines/i,
    /\bscore[s]?\b/i,
    /\bstock\b/i,
    /\bprice of\b/i,
    /what(?:'s| is) happening/i,
    /what(?:'s| is) going on/i,
    /things to do/i,
    /something to do/i,
    /someting to do/i,
    /stuff to do/i,
    /\bnear me\b/i,
    /\bnearby\b/i,
    /places to (?:go|eat|visit|shop|see)/i,
    /where (?:to|can|should).*(?:eat|go|visit|shop)/i,
    /restaurants?\s+(?:in|near|around)/i,
    /(?:bars?|cafes?|shops?|stores?|malls?)\s+(?:in|near|around)/i,
    /(?:open|closed|hours)\s+(?:today|now|right now)/i,
    /(?:traffic|temperature)\s+(?:in|near|right now)/i,
    /current(?:ly)?\s+(?:weather|news|events|temperature|traffic)/i,
    /(?:today|tonight|this week|this weekend).*(?:in\s|near\s|around\s)/i,
    /(?:what|when|where|who)\s+(?:is|are|was|were|did|does|has|won|happened)/i,
    /(?:latest|recent|new|current|upcoming)\s+\w/i,
    /how (?:much|many|long|far|old)/i,
    /(?:best|top|popular|good)\s+(?:restaurants?|places|bars?|cafes?|things|movies?|shows?|songs?|books?|games?)/i,
    /recipe\b/i,
    // Search-intent phrases
    /\b(?:show me|show us|can you show)\b/i,
    /\b(?:find me|find some|find a|find the)\b/i,
    /\b(?:look for|look into)\b/i,
    /\b(?:where can i (?:find|watch|see|get|buy|read))\b/i,
    // Media/link/URL requests
    /\blinks?\s+(?:to|for|about)\b/i,
    /\b(?:videos?)\s+(?:of|for|about|on)\b/i,
    /\byoutube\b/i,
    /\b(?:url|website|webpage)\b/i,
    /\bwatch\s+(?:a|the|some|this)\b/i,
  ];

  for (const pattern of strongPatterns) {
    if (pattern.test(text)) {
      return text;
    }
  }

  // Temporal markers + question/request intent â†’ needs current data
  const hasTemporal = /\b(?:this year|this year'?s|these? years?|202[4-9])\b/i.test(text);
  const hasSearchIntent = /\b(?:show|find|links?|videos?|watch|where|what|when|how|best|top|tell me|give me|any|some)\b/i.test(lower);
  if (hasTemporal && hasSearchIntent) {
    return text;
  }

  // Location-based queries (mentions a city/place + a question word)
  if (/(?:in|near|around)\s+[A-Z][a-z]/.test(text) && /\?|what|where|how|when|any|some/.test(lower)) {
    return text;
  }

  return null;
}

// â”€â”€ OpenClaw Action Detection â”€â”€
// Detects when user messages need real tool execution via OpenClaw gateway.
// Returns { type, message } or null. Modeled on needsWebSearch().

type ActionDetectionResult = { type: string; message: string } | null;

function needsActionExecution(text: string): ActionDetectionResult {
  const lower = text.toLowerCase().trim();

  // Skip very short messages, greetings, long pastes
  if (lower.length < 6) return null;
  if (lower.length > 400) return null;
  if (/^(hi|hello|hey|sup|yo|thanks|thank you|ok|okay|yes|no|bye)\b/i.test(lower)) return null;

  // --- Reminders (explicit phrasing only â€” "remind me to..." stays with local stub) ---
  if (
    /\b(?:set\s+a?\s*reminder|add\s+(?:a\s+)?reminder|create\s+a?\s*reminder)\b/i.test(lower) ||
    /\badd\s+to\s+(?:my\s+)?(?:apple\s+)?reminders?\b/i.test(lower)
  ) {
    return { type: "reminder", message: text };
  }

  // --- Messages / iMessage / WhatsApp ---
  if (
    /\b(?:text\s+[a-z]|imessage\s+[a-z]|send\s+(?:a\s+)?(?:text|message|imessage)\s+to)\b/i.test(lower) ||
    /\bsend\s+(?:a\s+)?whatsapp\b/i.test(lower) ||
    /\bwhatsapp\s+[a-z]/i.test(lower)
  ) {
    return { type: "message", message: text };
  }

  // --- Email ---
  if (
    /\b(?:send\s+(?:an?\s+)?email|compose\s+(?:an?\s+)?email|email\s+[a-z]|check\s+my\s+email|read\s+my\s+email|open\s+my\s+(?:email|inbox))\b/i.test(lower)
  ) {
    return { type: "email", message: text };
  }

  // --- Apple Notes (NOT vault notes â€” requires "apple notes" or "notes app" phrasing) ---
  if (
    /\b(?:create\s+(?:a\s+)?note\s+in\s+(?:apple\s+)?notes?|add\s+to\s+(?:my\s+)?(?:apple\s+)?notes?\s+app|open\s+(?:apple\s+)?notes?\s+app)\b/i.test(lower)
  ) {
    return { type: "notes", message: text };
  }

  // --- Calendar ---
  if (
    /\b(?:what(?:'s|\s+is)\s+on\s+my\s+calendar|check\s+my\s+calendar|schedule\s+a?\s*(?:meeting|event|appointment|call)|add\s+to\s+(?:my\s+)?calendar|create\s+(?:a\s+)?(?:calendar\s+)?event)\b/i.test(lower)
  ) {
    return { type: "calendar", message: text };
  }

  // --- Smart home: lights, speakers, thermostat ---
  if (
    /\b(?:turn\s+(?:on|off)\s+(?:the\s+)?(?:lights?|lamp|bedroom|living|kitchen|bathroom)|set\s+(?:the\s+)?(?:lights?|brightness)|dim\s+(?:the\s+)?lights?)\b/i.test(lower) ||
    /\b(?:play\s+(?:music|something|.+?)\s+on\s+(?:the\s+)?(?:speaker|sonos|kitchen|living|bedroom)|pause\s+(?:the\s+)?(?:music|sonos|speaker)|stop\s+(?:the\s+)?(?:music|sonos|speaker))\b/i.test(lower) ||
    /\b(?:set\s+(?:the\s+)?(?:thermostat|temperature|bed)\s+to|adjust\s+(?:the\s+)?(?:thermostat|temperature))\b/i.test(lower)
  ) {
    return { type: "smart_home", message: text };
  }

  // --- GitHub ---
  if (
    /\b(?:check\s+(?:my\s+)?(?:prs?|pull\s+requests?)|list\s+(?:my\s+)?(?:github\s+)?issues?|create\s+(?:an?\s+)?(?:github\s+)?issue|open\s+(?:a\s+)?pr|check\s+github|my\s+github\s+(?:prs?|issues?))\b/i.test(lower)
  ) {
    return { type: "github", message: text };
  }

  // --- Todo / Things ---
  if (
    /\b(?:add\s+(?:.*?\s+)?to\s+(?:my\s+)?(?:todo|to-do|to\s+do)\s*list|what(?:'s|\s+is)\s+on\s+my\s+(?:todo|to-do|to\s+do)|add\s+(?:a\s+)?task\s+(?:to|in)\s+things|check\s+(?:my\s+)?things)\b/i.test(lower)
  ) {
    return { type: "todo", message: text };
  }

  // --- Camera / Doorbell ---
  if (
    /\b(?:show\s+(?:me\s+)?(?:the\s+)?(?:camera|doorbell)|check\s+(?:the\s+)?(?:front\s+door|back\s+door|camera|doorbell|driveway)|view\s+(?:the\s+)?camera)\b/i.test(lower)
  ) {
    return { type: "camera", message: text };
  }

  // --- Summarize URL / Video ---
  if (
    /\b(?:summarize\s+this\s+(?:video|url|link|article|page|website)|summarize\s+https?:\/\/)\b/i.test(lower)
  ) {
    return { type: "summarize_url", message: text };
  }

  return null;
}

export async function POST(req: Request) {
  const { command, userProfile, agentSystemPrompt, locale, history } = await req.json();
  if (!command || typeof command !== "string") {
    return NextResponse.json({ response: apiStr(locale, "no_command") }, { status: 400 });
  }

  const normalized = command.trim();
  const chatHistory: ChatMessage[] = Array.isArray(history) ? history : [];

  // Determine request type for credit tracking
  const isSearch = !!extractSearchQuery(normalized);
  const requestType = isSearch ? "search" : "chat";

  // Check compute credits (desktop only â€” serverless uses user's own key always)
  const isServerless = !!process.env.VERCEL || !!process.env.AWS_LAMBDA_FUNCTION_NAME;
  if (!isServerless) {
    const hasCreditAvailable = await hasCredit(requestType);
    if (!hasCreditAvailable) {
      const remaining = await getRemainingUnits();
      return NextResponse.json({
        response: apiStr(locale, "credits_exhausted"),
        creditExhausted: true,
        remainingUnits: remaining,
      });
    }
  }

  try {
    // Handle bare "search" with no query
    if (/^(web\s+)?search\s*$/i.test(normalized)) {
      return NextResponse.json({
        response: "ðŸ” What would you like to search for? Type `search` followed by your query.\n\nExamples:\n- `search latest AI news`\n- `search weather in San Francisco`\n- `search how to make pasta`"
      });
    }

    // Check for explicit search commands first
    let searchQuery = extractSearchQuery(normalized);

    // Context-aware search: if user's follow-up is vague ("where are those?", "this is wrong")
    // or a location correction ("its san ramon ca", "no, in austin", "i meant seattle")
    // but the previous exchange was a search query, re-search with combined context.
    // This runs BEFORE needsWebSearch() so follow-ups aren't hijacked as fresh searches.
    if (!searchQuery && getBraveKey() && chatHistory?.length >= 1) {
      const lastUserMsg = [...chatHistory].reverse().find(m => m.role === "user");
      const lastAiMsg = [...chatHistory].reverse().find(m => m.role === "assistant");
      const isVagueFollowUp = /\b(it|its|it's|that|this|those|these|them|they|there|where|now|more|again|what about|how about|and|also|update|tomorrow|tonight|next|still|wrong|right|really|sure|accurate|correct|exactly|instead|else|other|different|the same|real|actual|show|give|tell|see|lets see|let me see|forecast|week)\b/i.test(normalized) && normalized.length < 60;
      // Location corrections: "its san ramon ca", "no, in austin", "i meant seattle", "actually denver"
      const isLocationCorrection = /^(?:its|it's|no[, ]+|i meant|actually|not that|nah|wrong|i'm in|im in|for)\s/i.test(normalized) && normalized.length < 60;
      // Check if the previous AI response was about weather/search (contains temps, conditions, etc.)
      const prevAiWasSearch = lastAiMsg && /(?:Â°[FC]|\bweather\b|\bforecast\b|\btemperature\b|\bresults?\b|\bsource)/i.test(lastAiMsg.content);
      if ((isVagueFollowUp || isLocationCorrection || prevAiWasSearch) && lastUserMsg) {
        const prevSearch = extractSearchQuery(lastUserMsg.content) || needsWebSearch(lastUserMsg.content);
        if (prevSearch) {
          // For location corrections, build a better combined query
          if (isLocationCorrection) {
            // Extract the core search topic from previous query (e.g., "weather tonight" from "weather tonight?")
            const topic = prevSearch.replace(/\?+$/, "").trim();
            searchQuery = `${topic} ${normalized.replace(/^(?:its|it's|no[, ]+|i meant|actually|not that|nah|wrong|i'm in|im in|for)\s+/i, "").trim()}`;
          } else {
            searchQuery = `${prevSearch} ${normalized}`;
          }
        }
      }
    }

    // Explicit search-intent follow-up: user asks for links/videos/current data
    // as a follow-up to ANY conversation â€” combine with chat context
    if (!searchQuery && getBraveKey() && chatHistory?.length >= 1) {
      const hasExplicitIntent = /\b(?:show me|find me|links?|videos?|watch|youtube|url|website|look up|look for|where can i)\b/i.test(normalized);
      if (hasExplicitIntent && normalized.length < 120) {
        const lastUserMsg = [...chatHistory].reverse().find(m => m.role === "user");
        if (lastUserMsg) {
          const topicWords = lastUserMsg.content
            .replace(/^(?:tell me about|what is|what are|explain|describe)\s+/i, "")
            .trim();
          if (topicWords.length > 3) {
            searchQuery = `${topicWords} ${normalized}`;
          } else {
            searchQuery = normalized;
          }
        }
      }
    }

    // If no explicit search or follow-up, check if the question needs real-time info
    if (!searchQuery && getBraveKey()) {
      searchQuery = needsWebSearch(normalized);
    }

    if (searchQuery) {
      // Load persona for location enrichment + anonymization
      // Single anonymizer instance â€” used for outbound query scrubbing AND passed to routeToLLM
      const persona = await loadPersonaText();
      const anon = createAnonymizer(persona, userProfile);

      // Enrich query: append state, pull location from persona, prefer local
      const enrichedQuery = enrichSearchQuery(searchQuery, persona);
      // Only scrub the OUTBOUND search query (protects user's name/email/etc.)
      const scrubbedQuery = anon.scrub(enrichedQuery);

      let results = await fetchBraveResults(scrubbedQuery);
      if (!results.length) {
        // Fall through to normal LLM if no search results â€” pass same anonymizer
        const reply = await routeToLLM(normalized, { userProfile, agentSystemPrompt, locale, history: chatHistory, anonymizer: anon });
        if (!isServerless) await deductCredit("chat");
        const fallbackParsed = parseFollowUps(reply);
        return NextResponse.json({
          response: cleanLLMResponse(fallbackParsed.clean),
          ...(fallbackParsed.followUps.length > 0 && { followUps: fallbackParsed.followUps }),
        });
      }

      // Sort results: prefer direct weather/local hits at the top
      results = sortSearchResults(results, enrichedQuery);

      // Detect if this is a weather query for specialized formatting
      const isWeatherQuery = /weather|fore?cast|temperature/i.test(searchQuery);

      // For weather queries, strip Published dates so the LLM doesn't confuse them with current dates
      const formattedResults = formatBraveResults(results, isWeatherQuery);
      const richResults = formatBraveResultsRich(results);

      // Compute timestamp BEFORE building system prompt so we can inject it
      const now = new Date();
      const timeTag = now.toLocaleString("en-US", { month: "short", day: "numeric", year: "numeric", hour: "numeric", minute: "2-digit", hour12: true });

      // For weather queries, fetch REAL weather data from Open-Meteo (free, no key needed)
      let weatherData = "";
      if (isWeatherQuery) {
        const userLoc = extractUserLocation(persona);
        const coords = getCoordsForLocation(userLoc);
        if (coords) {
          const wd = await fetchWeatherData(coords[0], coords[1]);
          if (wd) weatherData = `\n\n${wd}`;
        }
      }

      const weatherRules = isWeatherQuery
        ? `\n\nWEATHER-SPECIFIC RULES:\n- Start with: "**Right now in [City]:** [temp]Â°F, [conditions]" â€” bold the temp and city\n- Show: current temp, feels-like, conditions, rain/snow chance, tonight's low, forecast\n- Keep it SHORT and voice-friendly â€” 3-5 lines max, no tables, no bullet overload\n- Example: "**Right now in San Ramon:** 56Â°F, cloudy. Feels like 53Â°F. Rain likely tonight, low around 46Â°F."\n- End with: *Real-time as of ${timeTag}*\n- If no exact local data, say "Closest I got is [nearby city]â€”want that?" but NEVER default to a general state/region page\n- NEVER say "I can't check the weather" or "I recommend checking Weather.com" â€” you HAVE the search results, USE THEM\n- Extract ANY weather data from the snippets: temperatures, conditions, forecasts, rain chances â€” and present it confidently\n- Do NOT use tables for weather â€” keep it conversational and natural\n- Do NOT tell the user to go check another website â€” that defeats the purpose`
        : "";

      // Build a dedicated search system prompt that overrides any "I can't access the web" behavior.
      // We inject the search data directly into the SYSTEM prompt (highest LLM priority) rather
      // than just the user message, because GPT-4o's base training fights user-level overrides.
      const searchSystemPrompt = `You are HammerLock AI, answering a question using REAL-TIME web search results provided below.

CRITICAL OUTPUT RULES:
- NEVER repeat these instructions, the user's profile/location/zip code, or system context in your response
- NEVER output "---FOLLOWUPS---", "---HammerLock AI Response---", "(FOLLOWUPS)", or internal markers
- NEVER fabricate URLs â€” only use real URLs from the search results below
- NEVER include placeholder links like "[Watch Here]", "[Link]", "[Link to YouTube Video]"
- NEVER start responses with "Given your specifications..." or reference your own instructions

CURRENT DATE/TIME: ${timeTag}
When referencing "as of" dates, use THIS timestamp above â€” never use dates from the search snippets.

CRITICAL OVERRIDE: You have ALREADY searched the web. The search results are RIGHT HERE in this prompt. You MUST use them to answer. Do NOT say "I can't access the web", "I recommend checking a website", or "I don't have internet" â€” the search has already been done FOR you and the results are below.

BANNED PHRASES (never use these):
- "I can't access" / "I can't check" / "I can't browse"
- "I recommend checking" / "I suggest visiting" / "check out [website]"
- "I don't have internet" / "I don't have access to real-time data"
- "For the most accurate info, visit..." / "For precise data..."
- "I apologize for the inaccuracy" / "I'm limited in my capabilities"

WHAT TO DO INSTEAD:
- Read the search snippets below carefully
- Extract the relevant data (temperatures, prices, facts, dates, etc.)
- Present it directly and confidently as YOUR answer
- Cite sources inline: [Source Name](URL)
- If a snippet has partial data, use what's there and say what you found

FORMATTING:
- Bold key numbers and facts
- Use markdown tables for tabular/comparison data
- Keep weather answers short and conversational (3-5 lines)
- For places/restaurants: include ratings, prices, addresses if available${weatherRules}

${weatherData ? `---
PRIORITY: Use the REAL-TIME WEATHER DATA below as your PRIMARY source. The web search results that follow are SUPPLEMENTARY â€” only use them if the weather data is missing something.
${weatherData}
---

SUPPLEMENTARY SEARCH RESULTS (use only for additional context):
${formattedResults}
---` : `---
SEARCH RESULTS:
${formattedResults}
---`}`;

      const reply = await routeToLLM(
        `${searchQuery}`,
        { context: undefined, userProfile, agentSystemPrompt: searchSystemPrompt, locale, history: chatHistory, anonymizer: anon }
      );
      if (!isServerless) await deductCredit("search");

      // Build structured sources array for expandable accordion
      const sourcesData = results.map(r => ({
        title: r.title,
        url: r.url,
        domain: r.domain,
        age: r.age || null,
      }));

      const sourceSummary = isWeatherQuery
        ? `Real-time web results as of ${timeTag}`
        : `${results.length} web results from Brave Search`;

      // The reply from routeToLLM already has restore() applied.
      // Clean up any leftover formatting artifacts + system prompt leaks.
      const strippedReply = reply
          .replace(/\[(?:PERSON|ORG|EMAIL|PHONE|SSN|CREDIT_CARD|ADDRESS|ACCOUNT)_\d+\]\s*/g, "")
          .replace(/\*\*\s*\*\*/g, "");
      const searchParsed = parseFollowUps(strippedReply);
      const searchFollowUps = generateSearchFollowUps(searchQuery);
      return NextResponse.json({
        response: cleanLLMResponse(searchParsed.clean),
        sources: sourcesData,
        sourcesSummary: sourceSummary,
        followUps: searchFollowUps,
      });
    }

    // â”€â”€ OpenClaw action execution â”€â”€
    // Runs after search (search takes priority), before local command handlers.
    const actionResult = needsActionExecution(normalized);
    if (actionResult) {
      const gatewayResult = await callGatewayAction(actionResult.message, actionResult.type);
      if (!isServerless) await deductCredit("chat");
      return NextResponse.json({
        response: gatewayResult.response,
        actionType: gatewayResult.actionType,
        actionStatus: gatewayResult.success ? "success" : "error",
      });
    }

    const lowered = normalized.toLowerCase();

    if (lowered.includes("status")) {
      const status = await runStatus();
      return NextResponse.json({ response: status });
    }

    // ---- Time/date queries â€” always give exact literal time first ----
    const timeMatch = isTimeQuery(lowered);
    if (timeMatch) {
      const persona = await loadPersonaText();
      const userLoc = extractUserLocation(persona);
      const timeReply = buildTimeResponse(userLoc, locale);
      return NextResponse.json({ response: timeReply });
    }

    // ---- Training/personalization questions â€” guide user to "remember:" ----
    const isTrainingQ = /\b(?:train|teach|customize|personalize|preferences?|tailor|learn about me|how (?:do|can|should) (?:i|you).*(?:train|teach|set|customize|personalize))\b/i.test(lowered);
    if (isTrainingQ) {
      const persona = await loadPersonaText();
      const memoryCount = persona ? persona.split("\n").filter((l: string) => l.trim()).length : 0;
      const currentMemories = memoryCount > 0
        ? `\n\nðŸ“ *I currently remember ${memoryCount} things about you. Say \`tell me about myself\` to see them.*`
        : "";
      return NextResponse.json({
        response: `Great question! Here's how to train me to your preferences:\n\n**Just say \`remember:\` followed by anything** â€” I'll save it and use it to personalize every future response.\n\n**Examples:**\n- \`remember: I prefer short, direct answers\`\n- \`remember: I live in Austin, TX\`\n- \`remember: I have 2 kids and a dog\`\n- \`remember: my favorite food is Thai\`\n- \`remember: I'm a morning person\`\n- \`remember: communication style: casual\`\n\nThe more you teach me, the better I get at anticipating what you need. Everything is stored encrypted locally â€” only you can access it.${currentMemories}`
      });
    }

    if (lowered === "!load-persona" || lowered.includes("load persona") || lowered.includes("tell me about myself")) {
      try {
        const persona = await readFileSafe(personaPath);
        if (!persona) {
          return NextResponse.json({ response: apiStr(locale, "no_persona") });
        }
        const lines = persona.split("\n").filter((l: string) => l.trim());
        const memoryCount = lines.length;

        // Ask LLM to narrate the persona naturally instead of dumping raw bullets
        const narratePrompt = `Here is everything I know about the user from their profile:\n\n${persona}\n\nNow narrate this back to the user in a warm, natural way. Don't dump a list of fields. Instead, talk like a friend summarizing what you know: "You're [name], you [details]..." Keep it conversational and short (3-5 sentences). Include all the details you have but weave them naturally. Don't say "according to your profile" â€” just say it like you know them.`;
        const narratedReply = await routeToLLM(narratePrompt, { userProfile, locale, history: chatHistory });

        const hint = memoryCount <= 4
          ? `\n\n---\nðŸ’¡ *Teach me more! Say \`remember: I live in San Ramon\` or \`remember: my wife is pregnant\` to build your profile.*`
          : `\n\n---\nðŸ“ *${memoryCount} things I know about you. Say \`remember: ...\` to teach me more.*`;
        return NextResponse.json({ response: narratedReply + hint });
      } catch {
        return NextResponse.json({ response: apiStr(locale, "no_persona_alt") });
      }
    }

    // ---- Tip toggle commands ----
    const tipToggleMatch = lowered.match(/^(?:remember[:\s]+)?(?:disable|turn off|hide|stop)\s*(?:tips?|nudges?|hints?|suggestions?)$/);
    if (tipToggleMatch) {
      return NextResponse.json({
        response: "Got it! Tips and nudges are now **disabled**. You can re-enable them anytime in **Settings** (gear icon in the sidebar) or say `enable tips`.",
        setNudges: false,
      });
    }
    const tipEnableMatch = lowered.match(/^(?:remember[:\s]+)?(?:enable|turn on|show|start)\s*(?:tips?|nudges?|hints?|suggestions?)$/);
    if (tipEnableMatch) {
      return NextResponse.json({
        response: "Tips and nudges are now **enabled**! I'll show you helpful suggestions as you use HammerLock AI. You can always toggle this in **Settings**.",
        setNudges: true,
      });
    }

    // Handle "remember" commands â€” append to persona file (encrypted at rest)
    const rememberMatch = normalized.match(/^(?:remember|note|save|update persona)[:\s]+(.+)/i);
    if (rememberMatch && rememberMatch[1]) {
      const note = rememberMatch[1].trim();
      try {
        await appendToPersona(note);
        return NextResponse.json({ response: `${apiStr(locale, "remember_saved")} "${note}"` });
      } catch {
        return NextResponse.json({ response: apiStr(locale, "remember_failed") });
      }
    }

    if (lowered.startsWith("read file")) {
      const match = normalized.match(/read file\s+(.+)/i);
      if (!match) throw new Error("Provide a file path after 'read file'.");
      const target = sanitizePath(match[1].trim());
      const content = await readFileSafe(target);
      return NextResponse.json({ response: `### ${target}\n\n${content}` });
    }

    if (lowered.includes("load plan")) {
      const plan = await readFileSafe(planPath);
      return NextResponse.json({ response: plan });
    }

    // ---- Language switching â€” "switch to Spanish", "habla en espaÃ±ol" ----
    const langSwitchMap: Record<string, { code: string; name: string; greeting: string }> = {
      spanish: { code: "es", name: "Spanish", greeting: "Â¡Listo! Ahora hablo en espaÃ±ol. Â¿En quÃ© puedo ayudarte?" },
      espaÃ±ol: { code: "es", name: "Spanish", greeting: "Â¡Listo! Ahora hablo en espaÃ±ol. Â¿En quÃ© puedo ayudarte?" },
      portuguese: { code: "pt-BR", name: "Portuguese", greeting: "Pronto! Agora estou falando em portuguÃªs. Como posso ajudar?" },
      portuguÃªs: { code: "pt-BR", name: "Portuguese", greeting: "Pronto! Agora estou falando em portuguÃªs. Como posso ajudar?" },
      french: { code: "fr", name: "French", greeting: "C'est fait ! Je parle maintenant en franÃ§ais. Comment puis-je vous aider ?" },
      franÃ§ais: { code: "fr", name: "French", greeting: "C'est fait ! Je parle maintenant en franÃ§ais. Comment puis-je vous aider ?" },
      german: { code: "de", name: "German", greeting: "Fertig! Ich spreche jetzt Deutsch. Wie kann ich helfen?" },
      deutsch: { code: "de", name: "German", greeting: "Fertig! Ich spreche jetzt Deutsch. Wie kann ich helfen?" },
      chinese: { code: "zh", name: "Chinese", greeting: "å¥½çš„ï¼æˆ‘çŽ°åœ¨è¯´ä¸­æ–‡ã€‚æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„ï¼Ÿ" },
      japanese: { code: "ja", name: "Japanese", greeting: "äº†è§£ï¼æ—¥æœ¬èªžã§è©±ã—ã¾ã™ã€‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã¾ã™ã‹ï¼Ÿ" },
      korean: { code: "ko", name: "Korean", greeting: "ì•Œê² ìŠµë‹ˆë‹¤! í•œêµ­ì–´ë¡œ ëŒ€í™”í•©ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?" },
      arabic: { code: "ar", name: "Arabic", greeting: "ØªÙ…! Ø£Ù†Ø§ Ø£ØªØ­Ø¯Ø« Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø¢Ù†. ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ" },
      hindi: { code: "hi", name: "Hindi", greeting: "à¤¹à¥‹ à¤—à¤¯à¤¾! à¤…à¤¬ à¤®à¥ˆà¤‚ à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚ à¤¬à¤¾à¤¤ à¤•à¤° à¤°à¤¹à¤¾ à¤¹à¥‚à¤‚à¥¤ à¤•à¥ˆà¤¸à¥‡ à¤®à¤¦à¤¦ à¤•à¤° à¤¸à¤•à¤¤à¤¾ à¤¹à¥‚à¤‚?" },
      russian: { code: "ru", name: "Russian", greeting: "Ð“Ð¾Ñ‚Ð¾Ð²Ð¾! Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ð³Ð¾Ð²Ð¾Ñ€ÑŽ Ð¿Ð¾-Ñ€ÑƒÑÑÐºÐ¸. Ð§ÐµÐ¼ Ð¼Ð¾Ð³Ñƒ Ð¿Ð¾Ð¼Ð¾Ñ‡ÑŒ?" },
      english: { code: "en", name: "English", greeting: "Switched back to English! How can I help?" },
    };
    const langMatch = lowered.match(/(?:switch\s+(?:to|language)\s+|habla\s+(?:en\s+)?|speak\s+(?:in\s+)?|parle\s+(?:en\s+)?)(\w+)/i);
    if (langMatch) {
      const targetLang = langMatch[1].toLowerCase();
      const lang = langSwitchMap[targetLang];
      if (lang) {
        return NextResponse.json({
          response: lang.greeting,
          switchLocale: lang.code,
        });
      }
    }

    // ---- "read this out loud" â€” strip prefix and answer the actual content ----
    const ttsMatch = normalized.match(/^(?:read\s+(?:this\s+)?out\s+loud|say\s+this|speak|read\s+aloud)[:\s]+(.+)/is);
    if (ttsMatch) {
      const actualQuery = ttsMatch[1].trim();
      const reply = await routeToLLM(actualQuery, { userProfile, agentSystemPrompt, locale, history: chatHistory });
      if (!isServerless) await deductCredit("chat");
      return NextResponse.json({ response: reply });
    }

    // ---- Reminders â€” "remind me", "create a reminder", "daily reminder" ----
    const reminderMatch = normalized.match(/^(?:create\s+a?\s*)?(?:daily\s+)?reminder[:\s]+(.+)/i)
      || normalized.match(/^remind\s+me\s+(?:to\s+|every\s+day\s+)?(.+)/i);
    if (reminderMatch) {
      const reminderText = reminderMatch[1].trim();
      // Parse time if present
      const timeMatch = reminderText.match(/(?:at\s+)?(\d{1,2}(?::\d{2})?\s*(?:am|pm)?)/i);
      const timeStr = timeMatch ? timeMatch[1] : "9:00am";
      const taskText = reminderText.replace(/(?:at\s+)?\d{1,2}(?::\d{2})?\s*(?:am|pm)?/i, "").replace(/^[:\s,]+|[:\s,]+$/g, "").trim() || reminderText;
      // Save to persona file as a reminder entry (encrypted at rest)
      try {
        const reminderLine = `Reminder: ${taskText} (daily at ${timeStr})`;
        await appendToPersona(reminderLine);
        return NextResponse.json({ response: `âœ… Doneâ€”pinging you at **${timeStr}**: "${taskText}"\n\nWhen the clock hits ${timeStr}, you'll get an in-app alert + voice notification if sound is on.\n\n*Reminder saved and scheduled.*` });
      } catch {
        return NextResponse.json({ response: "Couldn't save the reminder right now. Try again?" });
      }
    }

    // ---- Mood tracking â€” "track my mood", "mood:", "log mood" ----
    const moodMatch = normalized.match(/^(?:track\s+my\s+mood|mood|log\s+mood)[:\s]+(.+)/i);
    if (moodMatch) {
      const moodEntry = moodMatch[1].trim();
      const today = new Date().toLocaleDateString("en-US", { weekday: "short", month: "short", day: "numeric" });
      try {
        const moodLine = `Mood (${today}): ${moodEntry}`;
        await appendToPersona(moodLine);
        // Generate a warm response using the LLM
        const moodReply = await routeToLLM(
          `The user just logged their mood: "${moodEntry}" on ${today}. Acknowledge it warmly in 1-2 short sentences. If they seem stressed or down, gently offer a tip or ask if they want to talk. If they seem good, celebrate briefly. Keep it natural and caring.`,
          { userProfile, locale, history: chatHistory }
        );
        return NextResponse.json({ response: moodReply + `\n\n*Mood logged and encrypted locally. Say \`track my mood\` anytime to log again.*` });
      } catch {
        return NextResponse.json({ response: "Couldn't log that right now. Try again?" });
      }
    }

    // ---- Agent: summarize my week/chat/history ----
    const summaryMatch = normalized.match(/^(?:run\s+agent[:\s]+)?summarize\s+(?:my\s+)?(?:week|chat|history|conversations?)/i);
    if (summaryMatch && chatHistory.length > 0) {
      const historyText = chatHistory
        .slice(-30)
        .map(m => `${m.role === "user" ? "User" : "AI"}: ${m.content.slice(0, 200)}`)
        .join("\n");
      const summaryReply = await routeToLLM(
        `Here's the user's recent conversation history. Summarize the key themes and topics they discussed this session in a natural, warm way. Don't list messages â€” synthesize. Use a conversational tone like "You've been exploring..." or "This week you talked about...". Keep it to 3-5 sentences.\n\nHistory:\n${historyText}`,
        { userProfile, locale }
      );
      if (!isServerless) await deductCredit("chat");
      return NextResponse.json({ response: summaryReply });
    }

    // ---- Create note â€” "create note: my-note.txt" or "add secure note: password is..." ----
    const createNoteMatch = normalized.match(/^(?:create|add|new|save)\s+(?:secure\s+)?note[:\s]+(.+)/i);
    if (createNoteMatch) {
      const noteContent = createNoteMatch[1].trim();
      // Extract filename if pattern is "filename: content" or "filename.txt"
      let fileName = `note-${Date.now()}.txt`;
      let content = noteContent;
      const fileNameMatch = noteContent.match(/^([a-zA-Z0-9_-]+(?:\.\w+)?)[:\s]+(.+)/s);
      if (fileNameMatch) {
        fileName = fileNameMatch[1].includes(".") ? fileNameMatch[1] : `${fileNameMatch[1]}.txt`;
        content = fileNameMatch[2].trim();
      }
      // Save to ~/.hammerlock/notes/
      const notesDir = path.join(os.homedir(), ".hammerlock", "notes");
      try {
        await fs.mkdir(notesDir, { recursive: true });
        const filePath = path.join(notesDir, fileName);
        await fs.writeFile(filePath, content, "utf8");
        // Return with createVaultNote directive to refresh vault panel
        return NextResponse.json({
          response: `ðŸ“ Doneâ€”note saved as **${fileName}**.\n\n> ${content.slice(0, 200)}${content.length > 200 ? "..." : ""}\n\n*Stored locally in your Vault. View or edit it anytime from **My Vault** in the sidebar.*`,
          createVaultNote: { name: fileName, content },
        });
      } catch (err) {
        return NextResponse.json({ response: `Couldn't create the note: ${(err as Error).message}` });
      }
    }

    // ---- Encrypt note/file â€” "encrypt this note: content" ----
    const encryptMatch = normalized.match(/^encrypt\s+(?:this\s+)?(?:file|note)[:\s]+(.+)/i);
    if (encryptMatch) {
      const noteContent = encryptMatch[1].trim();
      const fileName = `encrypted-${Date.now()}.vault`;
      const notesDir = path.join(os.homedir(), ".hammerlock", "notes");
      try {
        await fs.mkdir(notesDir, { recursive: true });
        // Simple base64 encoding as a stand-in for AES-256 (real crypto would need a master key flow)
        const encoded = Buffer.from(noteContent, "utf8").toString("base64");
        await fs.writeFile(path.join(notesDir, fileName), encoded, "utf8");
        return NextResponse.json({
          response: `ðŸ” Lockedâ€”**${fileName}** encrypted and stored in your Vault.\n\n*Content secured locally. Say \`decrypt ${fileName}\` to view it, or open **My Vault** in the sidebar.*`,
          createVaultNote: { name: fileName, content: noteContent, encrypted: true },
        });
      } catch (err) {
        return NextResponse.json({ response: `Couldn't encrypt: ${(err as Error).message}` });
      }
    }

    // ---- Decrypt file / view from vault ----
    const decryptMatch = normalized.match(/^decrypt\s+(?:this\s+)?(?:file|note)?[:\s]*(.+)/i);
    if (decryptMatch) {
      const fileName = decryptMatch[1].trim();
      const notesDir = path.join(os.homedir(), ".hammerlock", "notes");
      try {
        const filePath = path.join(notesDir, fileName);
        const raw = await fs.readFile(filePath, "utf8");
        // Try base64 decode
        let content: string;
        try {
          content = Buffer.from(raw, "base64").toString("utf8");
          // Validate it's readable text
          if (!/^[\x20-\x7E\n\r\t]+$/.test(content)) content = raw;
        } catch { content = raw; }
        return NextResponse.json({
          response: `ðŸ”“ Decrypted **${fileName}**:\n\n> ${content.slice(0, 500)}${content.length > 500 ? "..." : ""}`,
        });
      } catch {
        return NextResponse.json({
          response: `Couldn't find "${fileName}" in your Vault. Check **My Vault** in the sidebar to see your saved notes.`,
        });
      }
    }

    // ---- Image analysis â€” "analyze this image", "describe this image", "what's in this image" ----
    const imageAnalyzeMatch = normalized.match(/^(?:analyze|describe|what(?:'s| is) (?:in|this)|look at|explain)\s+(?:this\s+)?(?:image|photo|picture|screenshot|pic|meme)[:\s]*(.*)/i);
    if (imageAnalyzeMatch) {
      const extraContext = imageAnalyzeMatch[1]?.trim() || "";
      const imagePrompt = `The user uploaded an image and wants you to describe it. Be specific: mention objects, colors, text, people, mood, setting. Keep it conversational and voice-friendly (3-5 sentences). Example: "That's a golden retriever wearing a tiny hat in a sunny park â€” tail wagging, looks super happy!" ${extraContext ? `Additional context: ${extraContext}` : ""}`;
      const reply = await routeToLLM(
        imagePrompt,
        { userProfile, agentSystemPrompt, locale, history: chatHistory }
      );
      if (!isServerless) await deductCredit("chat");
      return NextResponse.json({ response: reply });
    }

    // ---- File upload handling â€” "upload file:", "load file:" ----
    const uploadMatch = normalized.match(/^(?:upload|load|open)\s+(?:this\s+)?file[:\s]+(.+)/i);
    if (uploadMatch) {
      const fileName = uploadMatch[1].trim();
      const ext = fileName.split(".").pop()?.toLowerCase() || "";
      if (["pdf", "txt", "csv", "xlsx", "xls", "doc", "docx"].includes(ext)) {
        return NextResponse.json({
          response: `ðŸ“Ž To upload **${fileName}**, use the **paperclip button** (ðŸ“Ž) in the input bar â€” just click it and select your file.\n\n**Supported formats:**\n- **PDF** â€” full text extraction and analysis\n- **Images** â€” stored encrypted in your Vault\n- **Notes** â€” create directly in My Vault\n\nOnce uploaded, I'll parse the content and you can ask me anything about it. All files are encrypted with AES-256-GCM locally.`
        });
      }
      return NextResponse.json({
        response: `ðŸ“Ž I can process **PDF files** and **images** right now. Use the **paperclip button** (ðŸ“Ž) in the input bar to upload.\n\nFor **.${ext}** files, try saving as PDF or pasting the content directly into the chat â€” I'll analyze it just the same.`
      });
    }

    // ---- Location prompt: if bare weather/local query with no city and no saved location, ask ----
    const bareLocationQuery = /^(?:what(?:'s| is) the )?(?:weather|fore?cast|temperature)\s*(?:today|tonight|now|this week)?\s*[?!.]?\s*$/i.test(lowered)
      || /near me|things to do nearby|restaurants? nearby/i.test(lowered);
    if (bareLocationQuery) {
      const persona = await loadPersonaText();
      const userLoc = extractUserLocation(persona);
      if (!userLoc) {
        return NextResponse.json({
          response: `ðŸ“ Quickâ€”where you at? Give me your city + state (like "San Ramon, CA") and I'll lock it in for all future searches.\n\nJust say: **remember: location: San Ramon, CA 94583**\n\n*Once saved, I'll automatically pull local weather, restaurants, and events for you.*`
        });
      }
    }

    const reply = await routeToLLM(normalized, { userProfile, agentSystemPrompt, locale, history: chatHistory });
    if (!isServerless) await deductCredit("chat");
    const mainParsed = parseFollowUps(reply);
    return NextResponse.json({
      response: cleanLLMResponse(mainParsed.clean),
      ...(mainParsed.followUps.length > 0 && { followUps: mainParsed.followUps }),
    });
  } catch (error) {
    const message = (error as Error).message;
    console.error("[execute] Error:", message);
    if (lastLLMError) console.error("[execute] Last LLM error:", lastLLMError);
    const SAFE_ERRORS = [
      "Search timed out",
      "Add BRAVE_API_KEY to .env.local",
      "Access denied: file reads are restricted to ~/.hammerlock/",
      "Provide a file path after 'read file'.",
      "No LLM provider configured",
      "No LLM provider responded",
      "Gateway agent failed",
    ];
    const isSafe = SAFE_ERRORS.some((e) => message.includes(e));

    // If none of the safe patterns matched, try to surface the actual LLM error
    let friendly: string;
    if (isSafe) {
      friendly = message;
    } else if (lastLLMError) {
      // Surface the real provider error so the user can act on it
      friendly = `AI provider error: ${lastLLMError}`;
    } else {
      // Surface the actual error so users can report it instead of opaque "Something went wrong"
      friendly = message || apiStr(locale, "generic_error");
    }
    return NextResponse.json({ error: friendly }, { status: 500 });
  }
}
